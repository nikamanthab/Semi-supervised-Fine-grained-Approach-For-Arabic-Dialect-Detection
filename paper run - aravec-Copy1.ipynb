{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Linear,Softmax,CrossEntropyLoss,Module,ReLU,DataParallel,Sequential\n",
    "from torch.optim import Adam, SGD\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "from pymagnitude import *\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(file_name, preds):\n",
    "    with open('../results/'+file_name, 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    with open('../results/aravec_result.txt', 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    !python ../NADI-2020_release_1.0/NADI_release/NADI-DID-Scorer.py ../tsv/gold1.txt ../results/aravec_result.txt\n",
    "\n",
    "def write_result2(file_name, preds):\n",
    "    with open('../results/'+file_name, 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    with open('../results/aravec_result.txt', 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    !python ../NADI-2020_release_1.0/NADI_release/NADI-DID-Scorer.py ../tsv/gold2.txt ../results/aravec_result.txt\n",
    "\n",
    "def preprocess_text(train_list, test_list):\n",
    "    X_train_corrected_tweets = []\n",
    "    for tweet in tqdm(train_list):\n",
    "        new_tweet = re.findall( '[^A-Za-z:/_.0-9\\\\#@,=+\\(\\)]+' ,tweet)\n",
    "        new_tweet = \" \".join(new_tweet).replace('\\xa0','').replace('\\u200c','').replace('\\U000fe329','').replace('\\u2066','').replace('\\u2069','').strip()\n",
    "        X_train_corrected_tweets.append(new_tweet)\n",
    "\n",
    "    X_dev_corrected_tweets = []\n",
    "    for tweet in tqdm(test_list):\n",
    "        new_tweet = re.findall( '[^A-Za-z:/_.0-9\\\\#@,=+\\(\\)]+' ,tweet) #[^\\x00-\\x19\\x21-\\x7F]+\n",
    "        new_tweet = \" \".join(new_tweet).replace('\\xa0','').replace('\\u200c','').replace('\\U000fe329','').replace('\\u2066','').replace('\\u2069','').strip()\n",
    "        X_dev_corrected_tweets.append(new_tweet)\n",
    "    return X_train_corrected_tweets, X_dev_corrected_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aravec\n",
    "t_model = gensim.models.Word2Vec.load(\n",
    "    '../downloads/aravec/full_uni_cbow_100_twitter/full_uni_cbow_100_twitter.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDataset(Dataset):\n",
    "    def __init__(self, X,y):\n",
    "        self.text = X\n",
    "        self.label = y\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'text':self.text[idx],\n",
    "            'label':torch.from_numpy(label_onehot(self.label[idx])).to(device)\n",
    "            }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#3 country_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#3 country_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "y_train_original = train_df[\"#3 country_label\"]\n",
    "for u in range(len(y_train_original.unique())):\n",
    "    label_map[y_train_original.unique()[u]] = int(u)\n",
    "reverse_label_map = {value : key for (key, value) in label_map.items()}\n",
    "def label_onehot(label):\n",
    "    onehot = np.zeros((21))\n",
    "    index = label_map[label]\n",
    "    onehot[index] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n"
     ]
    }
   ],
   "source": [
    "X_noov = []\n",
    "for sentence in X_train_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov.append(new_sentence)\n",
    "X_noov_dev = []\n",
    "for sentence in X_dev_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov_dev.append(new_sentence)\n",
    "    \n",
    "X_train = []\n",
    "counter = 0\n",
    "for one_vec in X_noov:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_train.append(word_vector)\n",
    "print(counter)\n",
    "X_dev = []\n",
    "counter = 0\n",
    "for one_vec in X_noov_dev:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_dev.append(word_vector)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuningNet(Module):\n",
    "    def __init__(self, D_in, H,D_out):\n",
    "        super(TuningNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.relu = ReLU()\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        self.softmax = Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        y_pred = self.softmax(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_epochs = 10\n",
    "learning_rate = 0.001\n",
    "model = TuningNet(100,512,21).to(device)\n",
    "train_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = ArabicDataset(X_train,y_train_original)\n",
    "trainloader = DataLoader(traindataset, batch_size=32,\n",
    "                        shuffle=True)\n",
    "# testloader = DataLoader(traindataset, batch_size=1000)\n",
    "devdataset = ArabicDataset(X_dev, y_dev_original)\n",
    "devloader = DataLoader(devdataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 21/657 [00:00<00:03, 203.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:03<00:00, 218.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.26671428571428574 training f1_score: 0.04939007765483354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 23/657 [00:00<00:02, 229.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.28525317732499494 training f1_score: 0.05058628706912257\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:02<00:00, 238.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.29128571428571426 training f1_score: 0.05535979214443271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 24/657 [00:00<00:02, 233.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.2991728868267097 training f1_score: 0.05706707778205363\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:02<00:00, 233.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.29114285714285715 training f1_score: 0.055866462573108296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 23/657 [00:00<00:02, 224.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.3114787169659068 training f1_score: 0.05976696905965199\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:04<00:00, 139.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.2978571428571429 training f1_score: 0.0570758195559946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 24/657 [00:00<00:02, 236.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.2971555376235626 training f1_score: 0.053899592132051075\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:03<00:00, 212.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.29219047619047617 training f1_score: 0.0543660667947843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 23/657 [00:00<00:02, 229.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.3064353439580391 training f1_score: 0.05550428961365185\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:02<00:00, 244.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.2977619047619048 training f1_score: 0.05482412823461103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 17/657 [00:00<00:03, 167.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.31309259632842446 training f1_score: 0.05791849573106977\n",
      "epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:03<00:00, 212.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.30147619047619045 training f1_score: 0.05567926502871911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 22/657 [00:00<00:02, 218.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.30038329634859795 training f1_score: 0.054472297505697445\n",
      "epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:03<00:00, 213.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.29928571428571427 training f1_score: 0.0558191050770529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 22/657 [00:00<00:02, 219.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.30340932015331856 training f1_score: 0.058791596852704665\n",
      "epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:03<00:00, 189.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.3028095238095238 training f1_score: 0.05626823679731235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 23/657 [00:00<00:02, 226.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.29453298365947145 training f1_score: 0.0515081410052382\n",
      "epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:02<00:00, 247.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.30552380952380953 training f1_score: 0.056344521676059175\n",
      "dev acc: 0.28142021383901555 training f1_score: 0.046885301922708886\n"
     ]
    }
   ],
   "source": [
    "criterian = CrossEntropyLoss().to(device)\n",
    "# optimizer = SGD(model.parameters(), lr=learning_rate,momentum=0.9,nesterov=True)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "train_f1 = []\n",
    "dev_f1 = []\n",
    "for epoch in range(num_of_epochs):\n",
    "    i_batch = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    print(\"epoch:\",epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        i_batch +=1\n",
    "        output = model(batch['text'].to(device))\n",
    "        loss = criterian(output,batch['label'].argmax(dim=1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred += list(output.argmax(dim=1).detach().cpu().numpy())\n",
    "        y_true += list(batch['label'].argmax(dim=1).detach().cpu().numpy()) \n",
    "    print(\"training acc:\",accuracy_score(y_pred,y_true),end=' ')\n",
    "    f1 = f1_score(y_pred,y_true,average='macro')\n",
    "    train_f1.append(f1)\n",
    "    print(\"training f1_score:\", f1)\n",
    "    \n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for batch in devloader:\n",
    "        output = model(batch['text'].to(device))\n",
    "        y_pred += list(output.argmax(dim=1).detach().cpu().numpy())\n",
    "        y_true += list(batch['label'].argmax(dim=1).detach().cpu().numpy())\n",
    "    print(\"dev acc:\",accuracy_score(y_pred,y_true),end=' ')\n",
    "    f1 = f1_score(y_pred,y_true,average='macro')\n",
    "    dev_f1.append(f1)\n",
    "    print(\"training f1_score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\r\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\r\n",
      "\r\n",
      "OVERALL SCORES:\r\n",
      "MACRO AVERAGE PRECISION SCORE: 3.81 %\r\n",
      "MACRO AVERAGE RECALL SCORE: 7.48 %\r\n",
      "MACRO AVERAGE F1 SCORE: 4.69 %\r\n",
      "OVERALL ACCURACY: 28.14 %\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "pred = [reverse_label_map[i] for i in y_pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result('aravec_task1_unbal_try.txt',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../tsv/oversample_train.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#3 country_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#3 country_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=21)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1309\n"
     ]
    }
   ],
   "source": [
    "X_noov = []\n",
    "for sentence in X_train_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov.append(new_sentence)\n",
    "X_noov_dev = []\n",
    "for sentence in X_dev_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov_dev.append(new_sentence)\n",
    "    \n",
    "X_train = []\n",
    "counter = 0\n",
    "for one_vec in X_noov:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_train.append(word_vector)\n",
    "print(counter)\n",
    "X_dev = []\n",
    "counter = 0\n",
    "for one_vec in X_noov_dev:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_dev.append(word_vector)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 93933 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "93933/93933 [==============================] - 4s 40us/step - loss: 2.3750 - accuracy: 0.3803 - val_loss: 3.2318 - val_accuracy: 0.1650\n",
      "Epoch 2/50\n",
      "93933/93933 [==============================] - 4s 40us/step - loss: 1.4616 - accuracy: 0.5844 - val_loss: 3.5083 - val_accuracy: 0.1832\n",
      "Epoch 3/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 1.1686 - accuracy: 0.6677 - val_loss: 4.0113 - val_accuracy: 0.2021\n",
      "Epoch 4/50\n",
      "93933/93933 [==============================] - 4s 42us/step - loss: 1.0072 - accuracy: 0.7160 - val_loss: 4.4454 - val_accuracy: 0.2011\n",
      "Epoch 5/50\n",
      "93933/93933 [==============================] - 4s 42us/step - loss: 0.8847 - accuracy: 0.7525 - val_loss: 4.9604 - val_accuracy: 0.2033\n",
      "Epoch 6/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 0.8154 - accuracy: 0.7755 - val_loss: 5.4539 - val_accuracy: 0.2140\n",
      "Epoch 7/50\n",
      "93933/93933 [==============================] - 4s 43us/step - loss: 0.7433 - accuracy: 0.7954 - val_loss: 6.0215 - val_accuracy: 0.1783\n",
      "Epoch 8/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 0.7046 - accuracy: 0.8091 - val_loss: 6.2856 - val_accuracy: 0.1801\n",
      "Epoch 9/50\n",
      "93933/93933 [==============================] - 4s 42us/step - loss: 0.6541 - accuracy: 0.8247 - val_loss: 6.5983 - val_accuracy: 0.1995\n",
      "Epoch 10/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 0.6236 - accuracy: 0.8334 - val_loss: 7.0566 - val_accuracy: 0.2031\n",
      "Epoch 11/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 0.6044 - accuracy: 0.8394 - val_loss: 7.4864 - val_accuracy: 0.2084\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(21, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=10, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Lebanon', 1: 'Sudan', 2: 'Djibouti', 3: 'Saudi_Arabia', 4: 'Oman', 5: 'Yemen', 6: 'Bahrain', 7: 'Mauritania', 8: 'Kuwait', 9: 'Iraq', 10: 'Palestine', 11: 'Jordan', 12: 'Somalia', 13: 'Tunisia', 14: 'Morocco', 15: 'Syria', 16: 'United_Arab_Emirates', 17: 'Algeria', 18: 'Egypt', 19: 'Qatar', 20: 'Libya'}\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 12.82 %\n",
      "MACRO AVERAGE RECALL SCORE: 12.02 %\n",
      "MACRO AVERAGE F1 SCORE: 10.47 %\n",
      "OVERALL ACCURACY: 16.50 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result('aravec_task1_bal.txt',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c4d409193d406489f9e4b08df9496c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv','\\t')\n",
    "g = train_df['#4 province_label'].value_counts()[0]\n",
    "for i in tqdm_notebook(train_df['#4 province_label'].unique()):\n",
    "    filter_df = train_df[train_df['#4 province_label'] == i]\n",
    "    count = len(filter_df)\n",
    "    while (count<=g):\n",
    "        count+=1\n",
    "        idx = random.randint(0,len(filter_df)-1)\n",
    "        train_df = pd.concat([train_df,filter_df[idx:idx+1]])\n",
    "train_df['#4 province_label'].value_counts()\n",
    "train_df.to_csv('../tsv/oversample_train2.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#4 province_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#4 province_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=100)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n"
     ]
    }
   ],
   "source": [
    "X_noov = []\n",
    "for sentence in X_train_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov.append(new_sentence)\n",
    "X_noov_dev = []\n",
    "for sentence in X_dev_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov_dev.append(new_sentence)\n",
    "    \n",
    "X_train = []\n",
    "counter = 0\n",
    "for one_vec in X_noov:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_train.append(word_vector)\n",
    "print(counter)\n",
    "X_dev = []\n",
    "counter = 0\n",
    "for one_vec in X_noov_dev:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_dev.append(word_vector)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "21000/21000 [==============================] - 1s 69us/step - loss: 5.4428 - accuracy: 0.0354 - val_loss: 4.7153 - val_accuracy: 0.0242\n",
      "Epoch 2/50\n",
      "21000/21000 [==============================] - 1s 65us/step - loss: 4.1859 - accuracy: 0.0777 - val_loss: 4.7348 - val_accuracy: 0.0270\n",
      "Epoch 3/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 3.9014 - accuracy: 0.1180 - val_loss: 4.7843 - val_accuracy: 0.0309\n",
      "Epoch 4/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 3.6782 - accuracy: 0.1530 - val_loss: 4.9268 - val_accuracy: 0.0343\n",
      "Epoch 5/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 3.4886 - accuracy: 0.1867 - val_loss: 5.1040 - val_accuracy: 0.0375\n",
      "Epoch 6/50\n",
      "21000/21000 [==============================] - 1s 68us/step - loss: 3.3198 - accuracy: 0.2178 - val_loss: 5.2999 - val_accuracy: 0.0299\n",
      "Epoch 7/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 3.1421 - accuracy: 0.2505 - val_loss: 5.5502 - val_accuracy: 0.0329\n",
      "Epoch 8/50\n",
      "21000/21000 [==============================] - 1s 69us/step - loss: 2.9742 - accuracy: 0.2831 - val_loss: 5.8192 - val_accuracy: 0.0337\n",
      "Epoch 9/50\n",
      "21000/21000 [==============================] - 1s 67us/step - loss: 2.8381 - accuracy: 0.3105 - val_loss: 6.0297 - val_accuracy: 0.0327\n",
      "Epoch 10/50\n",
      "21000/21000 [==============================] - 1s 66us/step - loss: 2.6746 - accuracy: 0.3478 - val_loss: 6.3267 - val_accuracy: 0.0341\n",
      "Epoch 11/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 2.5506 - accuracy: 0.3680 - val_loss: 6.6905 - val_accuracy: 0.0313\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=10, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'iq_Al-Anbar', 1: 'eg_Alexandria', 2: 'iq_Maysan', 3: 'ma_Oriental', 4: 'ly_Al-Jabal-al-Akhdar', 5: 'ae_Fujairah', 6: 'eg_Ismailia', 7: 'iq_Baghdad', 8: 'eg_Dakahlia', 9: 'mr_Nouakchott', 10: 'eg_Qena', 11: 'ma_Marrakech-Tensift-Al-Haouz', 12: 'sa_Tabuk', 13: 'eg_Asyut', 14: 'iq_Karbala', 15: 'bh_Capital', 16: 'sy_Damascus-City', 17: 'sa_Najran', 18: 'dj_Djibouti', 19: 'lb_Akkar', 20: 'om_Musandam', 21: 'ps_Gaza-Strip', 22: 'dz_Oran', 23: 'so_Banaadir', 24: 'sy_As-Suwayda', 25: 'eg_Faiyum', 26: 'jo_Aqaba', 27: 'eg_Cairo', 28: 'lb_North-Lebanon', 29: 'eg_Port-Said', 30: 'eg_Monufia', 31: 'tn_Sousse', 32: 'eg_Beheira', 33: 'sa_Ash-Sharqiyah', 34: 'eg_Gharbia', 35: 'ae_Ras-Al-Khaymah', 36: 'eg_Minya', 37: 'om_Al-Batnah', 38: 'kw_Jahra', 39: 'dz_Jijel', 40: 'dz_Béchar', 41: 'eg_Sohag', 42: 'sy_Hims', 43: 'iq_An-Najaf', 44: 'lb_South-Lebanon', 45: 'eg_Aswan', 46: 'ye_Ibb', 47: 'iq_Dihok', 48: 'sy_Aleppo', 49: 'iq_Al-Muthannia', 50: 'dz_Bordj-Bou-Arreridj\\u200e', 51: 'ly_Tripoli', 52: 'ye_Dhamar', 53: 'ly_Misrata', 54: 'tn_Ariana', 55: 'ma_Meknes-Tafilalet', 56: 'iq_Wasit', 57: \"sa_Ha'il\", 58: 'sa_Jizan', 59: 'sa_Al-Quassim', 60: 'om_Muscat', 61: 'ly_Al-Butnan', 62: 'eg_Beni-Suef', 63: 'ae_Dubai', 64: 'eg_Suez', 65: 'ly_Benghazi', 66: 'eg_Red-Sea', 67: 'eg_North-Sinai', 68: 'sd_Khartoum', 69: 'eg_South-Sinai', 70: 'tn_Mahdia', 71: 'ma_Tanger-Tetouan', 72: 'ae_Abu-Dhabi', 73: 'om_Dhofar', 74: 'iq_As-Sulaymaniyah', 75: 'iq_Basra', 76: 'om_Ad-Dakhiliyah', 77: 'dz_Khenchela', 78: 'jo_Zarqa', 79: 'iq_Arbil', 80: 'sa_Al-Madinah', 81: 'eg_Kafr-el-Sheikh', 82: 'sa_Ar-Riyad', 83: 'kw_Hawalli', 84: 'dz_Ouargla', 85: 'ye_Al-Hudaydah', 86: 'tn_Kairouan', 87: 'ps_West-Bank', 88: 'qa_Ar-Rayyan', 89: 'sa_Asir', 90: 'ma_Souss-Massa-Draa', 91: 'iq_Ninawa', 92: 'eg_Luxor', 93: 'sy_Lattakia', 94: 'sa_Makkah', 95: 'om_Ash-Sharqiyah', 96: 'ye_Aden', 97: 'dz_Bouira', 98: 'ae_Umm-Al-Qaywayn', 99: 'qa_Doha'}\n",
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 2.15 %\n",
      "MACRO AVERAGE RECALL SCORE: 2.36 %\n",
      "MACRO AVERAGE F1 SCORE: 1.98 %\n",
      "OVERALL ACCURACY: 2.42 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result2('aravec_task2_unbal.txt',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../tsv/oversample_train2.tsv')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#4 province_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#4 province_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=100)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538\n"
     ]
    }
   ],
   "source": [
    "X_noov = []\n",
    "for sentence in X_train_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov.append(new_sentence)\n",
    "X_noov_dev = []\n",
    "for sentence in X_dev_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov_dev.append(new_sentence)\n",
    "    \n",
    "X_train = []\n",
    "counter = 0\n",
    "for one_vec in X_noov:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_train.append(word_vector)\n",
    "print(counter)\n",
    "X_dev = []\n",
    "counter = 0\n",
    "for one_vec in X_noov_dev:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_dev.append(word_vector)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39600 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "39600/39600 [==============================] - 3s 71us/step - loss: 4.7126 - accuracy: 0.0879 - val_loss: 4.8419 - val_accuracy: 0.0268\n",
      "Epoch 2/50\n",
      "39600/39600 [==============================] - 3s 64us/step - loss: 3.5838 - accuracy: 0.2004 - val_loss: 5.1013 - val_accuracy: 0.0305\n",
      "Epoch 3/50\n",
      "39600/39600 [==============================] - 3s 85us/step - loss: 3.1615 - accuracy: 0.2847 - val_loss: 5.4247 - val_accuracy: 0.0351\n",
      "Epoch 4/50\n",
      "39600/39600 [==============================] - 3s 71us/step - loss: 2.8233 - accuracy: 0.3561 - val_loss: 5.9098 - val_accuracy: 0.0367\n",
      "Epoch 5/50\n",
      "39600/39600 [==============================] - 3s 64us/step - loss: 2.5386 - accuracy: 0.4152 - val_loss: 6.3618 - val_accuracy: 0.0343\n",
      "Epoch 6/50\n",
      "39600/39600 [==============================] - 2s 62us/step - loss: 2.2948 - accuracy: 0.4685 - val_loss: 7.0649 - val_accuracy: 0.0347\n",
      "Epoch 7/50\n",
      "39600/39600 [==============================] - 3s 63us/step - loss: 2.1045 - accuracy: 0.5098 - val_loss: 7.5160 - val_accuracy: 0.0333\n",
      "Epoch 8/50\n",
      "39600/39600 [==============================] - 3s 63us/step - loss: 1.9532 - accuracy: 0.5461 - val_loss: 7.9556 - val_accuracy: 0.0341\n",
      "Epoch 9/50\n",
      "39600/39600 [==============================] - 2s 63us/step - loss: 1.8299 - accuracy: 0.5730 - val_loss: 8.5716 - val_accuracy: 0.0329\n",
      "Epoch 10/50\n",
      "39600/39600 [==============================] - 2s 62us/step - loss: 1.6992 - accuracy: 0.6011 - val_loss: 9.0053 - val_accuracy: 0.0323\n",
      "Epoch 11/50\n",
      "39600/39600 [==============================] - 2s 62us/step - loss: 1.6056 - accuracy: 0.6191 - val_loss: 9.6015 - val_accuracy: 0.0363\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=10, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'iq_Al-Anbar', 1: 'eg_Alexandria', 2: 'iq_Maysan', 3: 'ma_Oriental', 4: 'ly_Al-Jabal-al-Akhdar', 5: 'ae_Fujairah', 6: 'eg_Ismailia', 7: 'iq_Baghdad', 8: 'eg_Dakahlia', 9: 'mr_Nouakchott', 10: 'eg_Qena', 11: 'ma_Marrakech-Tensift-Al-Haouz', 12: 'sa_Tabuk', 13: 'eg_Asyut', 14: 'iq_Karbala', 15: 'bh_Capital', 16: 'sy_Damascus-City', 17: 'sa_Najran', 18: 'dj_Djibouti', 19: 'lb_Akkar', 20: 'om_Musandam', 21: 'ps_Gaza-Strip', 22: 'dz_Oran', 23: 'so_Banaadir', 24: 'sy_As-Suwayda', 25: 'eg_Faiyum', 26: 'jo_Aqaba', 27: 'eg_Cairo', 28: 'lb_North-Lebanon', 29: 'eg_Port-Said', 30: 'eg_Monufia', 31: 'tn_Sousse', 32: 'eg_Beheira', 33: 'sa_Ash-Sharqiyah', 34: 'eg_Gharbia', 35: 'ae_Ras-Al-Khaymah', 36: 'eg_Minya', 37: 'om_Al-Batnah', 38: 'kw_Jahra', 39: 'dz_Jijel', 40: 'dz_Béchar', 41: 'eg_Sohag', 42: 'sy_Hims', 43: 'iq_An-Najaf', 44: 'lb_South-Lebanon', 45: 'eg_Aswan', 46: 'ye_Ibb', 47: 'iq_Dihok', 48: 'sy_Aleppo', 49: 'iq_Al-Muthannia', 50: 'dz_Bordj-Bou-Arreridj\\u200e', 51: 'ly_Tripoli', 52: 'ye_Dhamar', 53: 'ly_Misrata', 54: 'tn_Ariana', 55: 'ma_Meknes-Tafilalet', 56: 'iq_Wasit', 57: \"sa_Ha'il\", 58: 'sa_Jizan', 59: 'sa_Al-Quassim', 60: 'om_Muscat', 61: 'ly_Al-Butnan', 62: 'eg_Beni-Suef', 63: 'ae_Dubai', 64: 'eg_Suez', 65: 'ly_Benghazi', 66: 'eg_Red-Sea', 67: 'eg_North-Sinai', 68: 'sd_Khartoum', 69: 'eg_South-Sinai', 70: 'tn_Mahdia', 71: 'ma_Tanger-Tetouan', 72: 'ae_Abu-Dhabi', 73: 'om_Dhofar', 74: 'iq_As-Sulaymaniyah', 75: 'iq_Basra', 76: 'om_Ad-Dakhiliyah', 77: 'dz_Khenchela', 78: 'jo_Zarqa', 79: 'iq_Arbil', 80: 'sa_Al-Madinah', 81: 'eg_Kafr-el-Sheikh', 82: 'sa_Ar-Riyad', 83: 'kw_Hawalli', 84: 'dz_Ouargla', 85: 'ye_Al-Hudaydah', 86: 'tn_Kairouan', 87: 'ps_West-Bank', 88: 'qa_Ar-Rayyan', 89: 'sa_Asir', 90: 'ma_Souss-Massa-Draa', 91: 'iq_Ninawa', 92: 'eg_Luxor', 93: 'sy_Lattakia', 94: 'sa_Makkah', 95: 'om_Ash-Sharqiyah', 96: 'ye_Aden', 97: 'dz_Bouira', 98: 'ae_Umm-Al-Qaywayn', 99: 'qa_Doha'}\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 2.53 %\n",
      "MACRO AVERAGE RECALL SCORE: 2.71 %\n",
      "MACRO AVERAGE F1 SCORE: 2.13 %\n",
      "OVERALL ACCURACY: 2.68 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result2('aravec_task2_unbal.txt',pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
