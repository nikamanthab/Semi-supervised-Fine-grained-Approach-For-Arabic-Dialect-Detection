{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.utils import to_categorical\n",
    "import gensim\n",
    "import numpy as np\n",
    "from keras.callbacks import callbacks\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(file_name, preds):\n",
    "    with open('../results/'+file_name, 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    with open('../results/bert_result.txt', 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    !python ../NADI-2020_release_1.0/NADI_release/NADI-DID-Scorer.py ../tsv/gold1.txt ../results/bert_result.txt\n",
    "\n",
    "def write_result2(file_name, preds):\n",
    "    with open('../results/'+file_name, 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    with open('../results/bert_result.txt', 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    !python ../NADI-2020_release_1.0/NADI_release/NADI-DID-Scorer.py ../tsv/gold2.txt ../results/bert_result.txt\n",
    "\n",
    "def preprocess_text(train_list, test_list):\n",
    "    X_train_corrected_tweets = []\n",
    "    for tweet in tqdm(train_list):\n",
    "        new_tweet = re.findall( '[^A-Za-z:/_.0-9\\\\#@,=+\\(\\)]+' ,tweet)\n",
    "        new_tweet = \" \".join(new_tweet).replace('\\xa0','').replace('\\u200c','').replace('\\U000fe329','').replace('\\u2066','').replace('\\u2069','').strip()\n",
    "        X_train_corrected_tweets.append(new_tweet)\n",
    "\n",
    "    X_dev_corrected_tweets = []\n",
    "    for tweet in tqdm(test_list):\n",
    "        new_tweet = re.findall( '[^A-Za-z:/_.0-9\\\\#@,=+\\(\\)]+' ,tweet) #[^\\x00-\\x19\\x21-\\x7F]+\n",
    "        new_tweet = \" \".join(new_tweet).replace('\\xa0','').replace('\\u200c','').replace('\\U000fe329','').replace('\\u2066','').replace('\\u2069','').strip()\n",
    "        X_dev_corrected_tweets.append(new_tweet)\n",
    "    return X_train_corrected_tweets, X_dev_corrected_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert - multilingual\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#3 country_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#3 country_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=21)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16831c07c504435e9750a56263160ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb62bacb3fb4c1d88c3d29e642e66af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-56f465cb2828>\", line 2, in <module>\n",
      "    X_train = model.encode(X_train_corrected)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\", line 150, in encode\n",
      "    out_features = self.forward(features)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 100, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sentence_transformers/models/Pooling.py\", line 66, in forward\n",
      "    output_vector = torch.cat(output_vectors, 1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "X_train_corrected, X_dev_corrected = preprocess_text(X_train_original, X_dev_original)\n",
    "X_train = model.encode(X_train_corrected,batch_size=32,)\n",
    "X_dev = model.encode(X_dev_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(21, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=10, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Iraq', 1: 'Egypt', 2: 'Morocco', 3: 'Libya', 4: 'United_Arab_Emirates', 5: 'Mauritania', 6: 'Saudi_Arabia', 7: 'Bahrain', 8: 'Syria', 9: 'Djibouti', 10: 'Lebanon', 11: 'Oman', 12: 'Palestine', 13: 'Algeria', 14: 'Somalia', 15: 'Jordan', 16: 'Tunisia', 17: 'Kuwait', 18: 'Yemen', 19: 'Sudan', 20: 'Qatar'}\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 13.21 %\n",
      "MACRO AVERAGE RECALL SCORE: 12.37 %\n",
      "MACRO AVERAGE F1 SCORE: 11.59 %\n",
      "OVERALL ACCURACY: 31.23 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result('bert_task1_unbal.txt',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../tsv/oversample_train.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#3 country_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#3 country_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=21)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1309\n"
     ]
    }
   ],
   "source": [
    "X_noov = []\n",
    "for sentence in X_train_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov.append(new_sentence)\n",
    "X_noov_dev = []\n",
    "for sentence in X_dev_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov_dev.append(new_sentence)\n",
    "    \n",
    "X_train = []\n",
    "counter = 0\n",
    "for one_vec in X_noov:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_train.append(word_vector)\n",
    "print(counter)\n",
    "X_dev = []\n",
    "counter = 0\n",
    "for one_vec in X_noov_dev:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_dev.append(word_vector)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 93933 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "93933/93933 [==============================] - 4s 40us/step - loss: 2.3750 - accuracy: 0.3803 - val_loss: 3.2318 - val_accuracy: 0.1650\n",
      "Epoch 2/50\n",
      "93933/93933 [==============================] - 4s 40us/step - loss: 1.4616 - accuracy: 0.5844 - val_loss: 3.5083 - val_accuracy: 0.1832\n",
      "Epoch 3/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 1.1686 - accuracy: 0.6677 - val_loss: 4.0113 - val_accuracy: 0.2021\n",
      "Epoch 4/50\n",
      "93933/93933 [==============================] - 4s 42us/step - loss: 1.0072 - accuracy: 0.7160 - val_loss: 4.4454 - val_accuracy: 0.2011\n",
      "Epoch 5/50\n",
      "93933/93933 [==============================] - 4s 42us/step - loss: 0.8847 - accuracy: 0.7525 - val_loss: 4.9604 - val_accuracy: 0.2033\n",
      "Epoch 6/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 0.8154 - accuracy: 0.7755 - val_loss: 5.4539 - val_accuracy: 0.2140\n",
      "Epoch 7/50\n",
      "93933/93933 [==============================] - 4s 43us/step - loss: 0.7433 - accuracy: 0.7954 - val_loss: 6.0215 - val_accuracy: 0.1783\n",
      "Epoch 8/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 0.7046 - accuracy: 0.8091 - val_loss: 6.2856 - val_accuracy: 0.1801\n",
      "Epoch 9/50\n",
      "93933/93933 [==============================] - 4s 42us/step - loss: 0.6541 - accuracy: 0.8247 - val_loss: 6.5983 - val_accuracy: 0.1995\n",
      "Epoch 10/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 0.6236 - accuracy: 0.8334 - val_loss: 7.0566 - val_accuracy: 0.2031\n",
      "Epoch 11/50\n",
      "93933/93933 [==============================] - 4s 41us/step - loss: 0.6044 - accuracy: 0.8394 - val_loss: 7.4864 - val_accuracy: 0.2084\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(21, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=10, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Lebanon', 1: 'Sudan', 2: 'Djibouti', 3: 'Saudi_Arabia', 4: 'Oman', 5: 'Yemen', 6: 'Bahrain', 7: 'Mauritania', 8: 'Kuwait', 9: 'Iraq', 10: 'Palestine', 11: 'Jordan', 12: 'Somalia', 13: 'Tunisia', 14: 'Morocco', 15: 'Syria', 16: 'United_Arab_Emirates', 17: 'Algeria', 18: 'Egypt', 19: 'Qatar', 20: 'Libya'}\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 12.82 %\n",
      "MACRO AVERAGE RECALL SCORE: 12.02 %\n",
      "MACRO AVERAGE F1 SCORE: 10.47 %\n",
      "OVERALL ACCURACY: 16.50 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result('aravec_task1_bal.txt',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c4d409193d406489f9e4b08df9496c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv','\\t')\n",
    "g = train_df['#4 province_label'].value_counts()[0]\n",
    "for i in tqdm_notebook(train_df['#4 province_label'].unique()):\n",
    "    filter_df = train_df[train_df['#4 province_label'] == i]\n",
    "    count = len(filter_df)\n",
    "    while (count<=g):\n",
    "        count+=1\n",
    "        idx = random.randint(0,len(filter_df)-1)\n",
    "        train_df = pd.concat([train_df,filter_df[idx:idx+1]])\n",
    "train_df['#4 province_label'].value_counts()\n",
    "train_df.to_csv('../tsv/oversample_train2.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#4 province_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#4 province_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=100)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n"
     ]
    }
   ],
   "source": [
    "X_noov = []\n",
    "for sentence in X_train_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov.append(new_sentence)\n",
    "X_noov_dev = []\n",
    "for sentence in X_dev_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov_dev.append(new_sentence)\n",
    "    \n",
    "X_train = []\n",
    "counter = 0\n",
    "for one_vec in X_noov:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_train.append(word_vector)\n",
    "print(counter)\n",
    "X_dev = []\n",
    "counter = 0\n",
    "for one_vec in X_noov_dev:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_dev.append(word_vector)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "21000/21000 [==============================] - 1s 69us/step - loss: 5.4428 - accuracy: 0.0354 - val_loss: 4.7153 - val_accuracy: 0.0242\n",
      "Epoch 2/50\n",
      "21000/21000 [==============================] - 1s 65us/step - loss: 4.1859 - accuracy: 0.0777 - val_loss: 4.7348 - val_accuracy: 0.0270\n",
      "Epoch 3/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 3.9014 - accuracy: 0.1180 - val_loss: 4.7843 - val_accuracy: 0.0309\n",
      "Epoch 4/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 3.6782 - accuracy: 0.1530 - val_loss: 4.9268 - val_accuracy: 0.0343\n",
      "Epoch 5/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 3.4886 - accuracy: 0.1867 - val_loss: 5.1040 - val_accuracy: 0.0375\n",
      "Epoch 6/50\n",
      "21000/21000 [==============================] - 1s 68us/step - loss: 3.3198 - accuracy: 0.2178 - val_loss: 5.2999 - val_accuracy: 0.0299\n",
      "Epoch 7/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 3.1421 - accuracy: 0.2505 - val_loss: 5.5502 - val_accuracy: 0.0329\n",
      "Epoch 8/50\n",
      "21000/21000 [==============================] - 1s 69us/step - loss: 2.9742 - accuracy: 0.2831 - val_loss: 5.8192 - val_accuracy: 0.0337\n",
      "Epoch 9/50\n",
      "21000/21000 [==============================] - 1s 67us/step - loss: 2.8381 - accuracy: 0.3105 - val_loss: 6.0297 - val_accuracy: 0.0327\n",
      "Epoch 10/50\n",
      "21000/21000 [==============================] - 1s 66us/step - loss: 2.6746 - accuracy: 0.3478 - val_loss: 6.3267 - val_accuracy: 0.0341\n",
      "Epoch 11/50\n",
      "21000/21000 [==============================] - 1s 64us/step - loss: 2.5506 - accuracy: 0.3680 - val_loss: 6.6905 - val_accuracy: 0.0313\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=10, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'iq_Al-Anbar', 1: 'eg_Alexandria', 2: 'iq_Maysan', 3: 'ma_Oriental', 4: 'ly_Al-Jabal-al-Akhdar', 5: 'ae_Fujairah', 6: 'eg_Ismailia', 7: 'iq_Baghdad', 8: 'eg_Dakahlia', 9: 'mr_Nouakchott', 10: 'eg_Qena', 11: 'ma_Marrakech-Tensift-Al-Haouz', 12: 'sa_Tabuk', 13: 'eg_Asyut', 14: 'iq_Karbala', 15: 'bh_Capital', 16: 'sy_Damascus-City', 17: 'sa_Najran', 18: 'dj_Djibouti', 19: 'lb_Akkar', 20: 'om_Musandam', 21: 'ps_Gaza-Strip', 22: 'dz_Oran', 23: 'so_Banaadir', 24: 'sy_As-Suwayda', 25: 'eg_Faiyum', 26: 'jo_Aqaba', 27: 'eg_Cairo', 28: 'lb_North-Lebanon', 29: 'eg_Port-Said', 30: 'eg_Monufia', 31: 'tn_Sousse', 32: 'eg_Beheira', 33: 'sa_Ash-Sharqiyah', 34: 'eg_Gharbia', 35: 'ae_Ras-Al-Khaymah', 36: 'eg_Minya', 37: 'om_Al-Batnah', 38: 'kw_Jahra', 39: 'dz_Jijel', 40: 'dz_Béchar', 41: 'eg_Sohag', 42: 'sy_Hims', 43: 'iq_An-Najaf', 44: 'lb_South-Lebanon', 45: 'eg_Aswan', 46: 'ye_Ibb', 47: 'iq_Dihok', 48: 'sy_Aleppo', 49: 'iq_Al-Muthannia', 50: 'dz_Bordj-Bou-Arreridj\\u200e', 51: 'ly_Tripoli', 52: 'ye_Dhamar', 53: 'ly_Misrata', 54: 'tn_Ariana', 55: 'ma_Meknes-Tafilalet', 56: 'iq_Wasit', 57: \"sa_Ha'il\", 58: 'sa_Jizan', 59: 'sa_Al-Quassim', 60: 'om_Muscat', 61: 'ly_Al-Butnan', 62: 'eg_Beni-Suef', 63: 'ae_Dubai', 64: 'eg_Suez', 65: 'ly_Benghazi', 66: 'eg_Red-Sea', 67: 'eg_North-Sinai', 68: 'sd_Khartoum', 69: 'eg_South-Sinai', 70: 'tn_Mahdia', 71: 'ma_Tanger-Tetouan', 72: 'ae_Abu-Dhabi', 73: 'om_Dhofar', 74: 'iq_As-Sulaymaniyah', 75: 'iq_Basra', 76: 'om_Ad-Dakhiliyah', 77: 'dz_Khenchela', 78: 'jo_Zarqa', 79: 'iq_Arbil', 80: 'sa_Al-Madinah', 81: 'eg_Kafr-el-Sheikh', 82: 'sa_Ar-Riyad', 83: 'kw_Hawalli', 84: 'dz_Ouargla', 85: 'ye_Al-Hudaydah', 86: 'tn_Kairouan', 87: 'ps_West-Bank', 88: 'qa_Ar-Rayyan', 89: 'sa_Asir', 90: 'ma_Souss-Massa-Draa', 91: 'iq_Ninawa', 92: 'eg_Luxor', 93: 'sy_Lattakia', 94: 'sa_Makkah', 95: 'om_Ash-Sharqiyah', 96: 'ye_Aden', 97: 'dz_Bouira', 98: 'ae_Umm-Al-Qaywayn', 99: 'qa_Doha'}\n",
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 2.15 %\n",
      "MACRO AVERAGE RECALL SCORE: 2.36 %\n",
      "MACRO AVERAGE F1 SCORE: 1.98 %\n",
      "OVERALL ACCURACY: 2.42 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result2('aravec_task2_unbal.txt',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../tsv/oversample_train2.tsv')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#4 province_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#4 province_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=100)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538\n"
     ]
    }
   ],
   "source": [
    "X_noov = []\n",
    "for sentence in X_train_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov.append(new_sentence)\n",
    "X_noov_dev = []\n",
    "for sentence in X_dev_original:\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in t_model.wv.vocab:\n",
    "            new_sentence.append(word)\n",
    "    X_noov_dev.append(new_sentence)\n",
    "    \n",
    "X_train = []\n",
    "counter = 0\n",
    "for one_vec in X_noov:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_train.append(word_vector)\n",
    "print(counter)\n",
    "X_dev = []\n",
    "counter = 0\n",
    "for one_vec in X_noov_dev:\n",
    "    if one_vec == []:\n",
    "        counter += 1\n",
    "        one_vec = ['ومايشوف']\n",
    "    word_vector = t_model.wv[ one_vec ]\n",
    "    word_vector = np.sum(word_vector,axis=0)\n",
    "#     word_vector = np.pad(word_vector,pad_width=((0,100-word_vector.shape[0]),(0,0)))\n",
    "    X_dev.append(word_vector)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39600 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "39600/39600 [==============================] - 3s 71us/step - loss: 4.7126 - accuracy: 0.0879 - val_loss: 4.8419 - val_accuracy: 0.0268\n",
      "Epoch 2/50\n",
      "39600/39600 [==============================] - 3s 64us/step - loss: 3.5838 - accuracy: 0.2004 - val_loss: 5.1013 - val_accuracy: 0.0305\n",
      "Epoch 3/50\n",
      "39600/39600 [==============================] - 3s 85us/step - loss: 3.1615 - accuracy: 0.2847 - val_loss: 5.4247 - val_accuracy: 0.0351\n",
      "Epoch 4/50\n",
      "39600/39600 [==============================] - 3s 71us/step - loss: 2.8233 - accuracy: 0.3561 - val_loss: 5.9098 - val_accuracy: 0.0367\n",
      "Epoch 5/50\n",
      "39600/39600 [==============================] - 3s 64us/step - loss: 2.5386 - accuracy: 0.4152 - val_loss: 6.3618 - val_accuracy: 0.0343\n",
      "Epoch 6/50\n",
      "39600/39600 [==============================] - 2s 62us/step - loss: 2.2948 - accuracy: 0.4685 - val_loss: 7.0649 - val_accuracy: 0.0347\n",
      "Epoch 7/50\n",
      "39600/39600 [==============================] - 3s 63us/step - loss: 2.1045 - accuracy: 0.5098 - val_loss: 7.5160 - val_accuracy: 0.0333\n",
      "Epoch 8/50\n",
      "39600/39600 [==============================] - 3s 63us/step - loss: 1.9532 - accuracy: 0.5461 - val_loss: 7.9556 - val_accuracy: 0.0341\n",
      "Epoch 9/50\n",
      "39600/39600 [==============================] - 2s 63us/step - loss: 1.8299 - accuracy: 0.5730 - val_loss: 8.5716 - val_accuracy: 0.0329\n",
      "Epoch 10/50\n",
      "39600/39600 [==============================] - 2s 62us/step - loss: 1.6992 - accuracy: 0.6011 - val_loss: 9.0053 - val_accuracy: 0.0323\n",
      "Epoch 11/50\n",
      "39600/39600 [==============================] - 2s 62us/step - loss: 1.6056 - accuracy: 0.6191 - val_loss: 9.6015 - val_accuracy: 0.0363\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=10, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'iq_Al-Anbar', 1: 'eg_Alexandria', 2: 'iq_Maysan', 3: 'ma_Oriental', 4: 'ly_Al-Jabal-al-Akhdar', 5: 'ae_Fujairah', 6: 'eg_Ismailia', 7: 'iq_Baghdad', 8: 'eg_Dakahlia', 9: 'mr_Nouakchott', 10: 'eg_Qena', 11: 'ma_Marrakech-Tensift-Al-Haouz', 12: 'sa_Tabuk', 13: 'eg_Asyut', 14: 'iq_Karbala', 15: 'bh_Capital', 16: 'sy_Damascus-City', 17: 'sa_Najran', 18: 'dj_Djibouti', 19: 'lb_Akkar', 20: 'om_Musandam', 21: 'ps_Gaza-Strip', 22: 'dz_Oran', 23: 'so_Banaadir', 24: 'sy_As-Suwayda', 25: 'eg_Faiyum', 26: 'jo_Aqaba', 27: 'eg_Cairo', 28: 'lb_North-Lebanon', 29: 'eg_Port-Said', 30: 'eg_Monufia', 31: 'tn_Sousse', 32: 'eg_Beheira', 33: 'sa_Ash-Sharqiyah', 34: 'eg_Gharbia', 35: 'ae_Ras-Al-Khaymah', 36: 'eg_Minya', 37: 'om_Al-Batnah', 38: 'kw_Jahra', 39: 'dz_Jijel', 40: 'dz_Béchar', 41: 'eg_Sohag', 42: 'sy_Hims', 43: 'iq_An-Najaf', 44: 'lb_South-Lebanon', 45: 'eg_Aswan', 46: 'ye_Ibb', 47: 'iq_Dihok', 48: 'sy_Aleppo', 49: 'iq_Al-Muthannia', 50: 'dz_Bordj-Bou-Arreridj\\u200e', 51: 'ly_Tripoli', 52: 'ye_Dhamar', 53: 'ly_Misrata', 54: 'tn_Ariana', 55: 'ma_Meknes-Tafilalet', 56: 'iq_Wasit', 57: \"sa_Ha'il\", 58: 'sa_Jizan', 59: 'sa_Al-Quassim', 60: 'om_Muscat', 61: 'ly_Al-Butnan', 62: 'eg_Beni-Suef', 63: 'ae_Dubai', 64: 'eg_Suez', 65: 'ly_Benghazi', 66: 'eg_Red-Sea', 67: 'eg_North-Sinai', 68: 'sd_Khartoum', 69: 'eg_South-Sinai', 70: 'tn_Mahdia', 71: 'ma_Tanger-Tetouan', 72: 'ae_Abu-Dhabi', 73: 'om_Dhofar', 74: 'iq_As-Sulaymaniyah', 75: 'iq_Basra', 76: 'om_Ad-Dakhiliyah', 77: 'dz_Khenchela', 78: 'jo_Zarqa', 79: 'iq_Arbil', 80: 'sa_Al-Madinah', 81: 'eg_Kafr-el-Sheikh', 82: 'sa_Ar-Riyad', 83: 'kw_Hawalli', 84: 'dz_Ouargla', 85: 'ye_Al-Hudaydah', 86: 'tn_Kairouan', 87: 'ps_West-Bank', 88: 'qa_Ar-Rayyan', 89: 'sa_Asir', 90: 'ma_Souss-Massa-Draa', 91: 'iq_Ninawa', 92: 'eg_Luxor', 93: 'sy_Lattakia', 94: 'sa_Makkah', 95: 'om_Ash-Sharqiyah', 96: 'ye_Aden', 97: 'dz_Bouira', 98: 'ae_Umm-Al-Qaywayn', 99: 'qa_Doha'}\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 2.53 %\n",
      "MACRO AVERAGE RECALL SCORE: 2.71 %\n",
      "MACRO AVERAGE F1 SCORE: 2.13 %\n",
      "OVERALL ACCURACY: 2.68 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result2('aravec_task2_unbal.txt',pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
