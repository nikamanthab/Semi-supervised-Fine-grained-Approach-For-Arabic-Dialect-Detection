{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor, BaggingClassifier, AdaBoostClassifier, VotingClassifier, VotingRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize, tokenize\n",
    "from pymagnitude import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Doc2Vec, Word2Vec\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from nltk import ngrams\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#3 country_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#3 country_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ec6a6f6c368c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#bert - multilingual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distiluse-base-multilingual-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "#bert - multilingual\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = model.encode(X_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = model.encode(X_dev_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_dev[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.63390603\n",
      "Iteration 2, loss = 2.43765121\n",
      "Iteration 3, loss = 2.34580507\n",
      "Iteration 4, loss = 2.28554455\n",
      "Iteration 5, loss = 2.23972196\n",
      "Iteration 6, loss = 2.19773230\n",
      "Iteration 7, loss = 2.15878706\n",
      "Iteration 8, loss = 2.12346746\n",
      "Iteration 9, loss = 2.08676281\n",
      "Iteration 10, loss = 2.05110874\n",
      "Iteration 11, loss = 2.01680581\n",
      "Iteration 12, loss = 1.98287927\n",
      "Iteration 13, loss = 1.94889402\n",
      "Iteration 14, loss = 1.91481393\n",
      "Iteration 15, loss = 1.88051607\n",
      "Iteration 16, loss = 1.84619045\n",
      "Iteration 17, loss = 1.81019658\n",
      "Iteration 18, loss = 1.77772584\n",
      "Iteration 19, loss = 1.74415940\n",
      "Iteration 20, loss = 1.70785432\n",
      "Iteration 21, loss = 1.67488785\n",
      "Iteration 22, loss = 1.64017454\n",
      "Iteration 23, loss = 1.60601141\n",
      "Iteration 24, loss = 1.57128622\n",
      "Iteration 25, loss = 1.53657603\n",
      "Iteration 26, loss = 1.50171965\n",
      "Iteration 27, loss = 1.46937019\n",
      "Iteration 28, loss = 1.43729439\n",
      "Iteration 29, loss = 1.40175660\n",
      "Iteration 30, loss = 1.37023968\n",
      "Iteration 31, loss = 1.33527984\n",
      "Iteration 32, loss = 1.30204977\n",
      "Iteration 33, loss = 1.27116621\n",
      "Iteration 34, loss = 1.23852617\n",
      "Iteration 35, loss = 1.20773234\n",
      "Iteration 36, loss = 1.17660509\n",
      "Iteration 37, loss = 1.14487325\n",
      "Iteration 38, loss = 1.11445708\n",
      "Iteration 39, loss = 1.08440946\n",
      "Iteration 40, loss = 1.05448342\n",
      "Iteration 41, loss = 1.02554078\n",
      "Iteration 42, loss = 0.99726576\n",
      "Iteration 43, loss = 0.96912256\n",
      "Iteration 44, loss = 0.94119480\n",
      "Iteration 45, loss = 0.91328515\n",
      "Iteration 46, loss = 0.88869408\n",
      "Iteration 47, loss = 0.86049142\n",
      "Iteration 48, loss = 0.83652846\n",
      "Iteration 49, loss = 0.81070315\n",
      "Iteration 50, loss = 0.78778008\n",
      "Iteration 51, loss = 0.76299939\n",
      "Iteration 52, loss = 0.73992610\n",
      "Iteration 53, loss = 0.71776254\n",
      "Iteration 54, loss = 0.69432041\n",
      "Iteration 55, loss = 0.67393390\n",
      "Iteration 56, loss = 0.65233020\n",
      "Iteration 57, loss = 0.63249362\n",
      "Iteration 58, loss = 0.61321716\n",
      "Iteration 59, loss = 0.59384137\n",
      "Iteration 60, loss = 0.57465852\n",
      "Iteration 61, loss = 0.55586683\n",
      "Iteration 62, loss = 0.53863780\n",
      "Iteration 63, loss = 0.51986675\n",
      "Iteration 64, loss = 0.50421927\n",
      "Iteration 65, loss = 0.48857338\n",
      "Iteration 66, loss = 0.47165205\n",
      "Iteration 67, loss = 0.45725359\n",
      "Iteration 68, loss = 0.44168552\n",
      "Iteration 69, loss = 0.42702308\n",
      "Iteration 70, loss = 0.41405854\n",
      "Iteration 71, loss = 0.40017478\n",
      "Iteration 72, loss = 0.38593658\n",
      "Iteration 73, loss = 0.37274904\n",
      "Iteration 74, loss = 0.36150120\n",
      "Iteration 75, loss = 0.34944585\n",
      "Iteration 76, loss = 0.33794143\n",
      "Iteration 77, loss = 0.32675196\n",
      "Iteration 78, loss = 0.31540778\n",
      "Iteration 79, loss = 0.30644072\n",
      "Iteration 80, loss = 0.29550002\n",
      "Iteration 81, loss = 0.28694230\n",
      "Iteration 82, loss = 0.27571875\n",
      "Iteration 83, loss = 0.26589264\n",
      "Iteration 84, loss = 0.25863541\n",
      "Iteration 85, loss = 0.24973231\n",
      "Iteration 86, loss = 0.24095218\n",
      "Iteration 87, loss = 0.23397040\n",
      "Iteration 88, loss = 0.22560850\n",
      "Iteration 89, loss = 0.22075304\n",
      "Iteration 90, loss = 0.21124126\n",
      "Iteration 91, loss = 0.20577452\n",
      "Iteration 92, loss = 0.19804516\n",
      "Iteration 93, loss = 0.19305438\n",
      "Iteration 94, loss = 0.18677077\n",
      "Iteration 95, loss = 0.18037652\n",
      "Iteration 96, loss = 0.17418750\n",
      "Iteration 97, loss = 0.16897934\n",
      "Iteration 98, loss = 0.16374346\n",
      "Iteration 99, loss = 0.15957589\n",
      "Iteration 100, loss = 0.15466526\n",
      "Iteration 101, loss = 0.14935592\n",
      "Iteration 102, loss = 0.14556331\n",
      "Iteration 103, loss = 0.14137992\n",
      "Iteration 104, loss = 0.13700250\n",
      "Iteration 105, loss = 0.13277137\n",
      "Iteration 106, loss = 0.12961495\n",
      "Iteration 107, loss = 0.12535690\n",
      "Iteration 108, loss = 0.12225997\n",
      "Iteration 109, loss = 0.11934801\n",
      "Iteration 110, loss = 0.11563150\n",
      "Iteration 111, loss = 0.11289002\n",
      "Iteration 112, loss = 0.10989773\n",
      "Iteration 113, loss = 0.10723077\n",
      "Iteration 114, loss = 0.10433095\n",
      "Iteration 115, loss = 0.10186871\n",
      "Iteration 116, loss = 0.09926763\n",
      "Iteration 117, loss = 0.09743628\n",
      "Iteration 118, loss = 0.09538074\n",
      "Iteration 119, loss = 0.09222692\n",
      "Iteration 120, loss = 0.08991010\n",
      "Iteration 121, loss = 0.08884591\n",
      "Iteration 122, loss = 0.08635051\n",
      "Iteration 123, loss = 0.08461172\n",
      "Iteration 124, loss = 0.08436246\n",
      "Iteration 125, loss = 0.08077734\n",
      "Iteration 126, loss = 0.07873254\n",
      "Iteration 127, loss = 0.07881642\n",
      "Iteration 128, loss = 0.07797620\n",
      "Iteration 129, loss = 0.07724673\n",
      "Iteration 130, loss = 0.07477365\n",
      "Iteration 131, loss = 0.07370596\n",
      "Iteration 132, loss = 0.07207363\n",
      "Iteration 133, loss = 0.06972572\n",
      "Iteration 134, loss = 0.06938109\n",
      "Iteration 135, loss = 0.06933775\n",
      "Iteration 136, loss = 0.06722892\n",
      "Iteration 137, loss = 0.06673059\n",
      "Iteration 138, loss = 0.06519209\n",
      "Iteration 139, loss = 0.06555995\n",
      "Iteration 140, loss = 0.06383776\n",
      "Iteration 141, loss = 0.06350422\n",
      "Iteration 142, loss = 0.06486151\n",
      "Iteration 143, loss = 0.06080660\n",
      "Iteration 144, loss = 0.06114088\n",
      "Iteration 145, loss = 0.06077122\n",
      "Iteration 146, loss = 0.06030962\n",
      "Iteration 147, loss = 0.06074392\n",
      "Iteration 148, loss = 0.05779081\n",
      "Iteration 149, loss = 0.05852593\n",
      "Iteration 150, loss = 0.05700254\n",
      "Iteration 151, loss = 0.05853359\n",
      "Iteration 152, loss = 0.05760063\n",
      "Iteration 153, loss = 0.05589433\n",
      "Iteration 154, loss = 0.05596968\n",
      "Iteration 155, loss = 0.05609575\n",
      "Iteration 156, loss = 0.05398510\n",
      "Iteration 157, loss = 0.05361171\n",
      "Iteration 158, loss = 0.05347679\n",
      "Iteration 159, loss = 0.05460299\n",
      "Iteration 160, loss = 0.05578923\n",
      "Iteration 161, loss = 0.05158139\n",
      "Iteration 162, loss = 0.05317416\n",
      "Iteration 163, loss = 0.05280271\n",
      "Iteration 164, loss = 0.05261368\n",
      "Iteration 165, loss = 0.05219113\n",
      "Iteration 166, loss = 0.05161546\n",
      "Iteration 167, loss = 0.05092892\n",
      "Iteration 168, loss = 0.05039041\n",
      "Iteration 169, loss = 0.05068183\n",
      "Iteration 170, loss = 0.05119912\n",
      "Iteration 171, loss = 0.04838151\n",
      "Iteration 172, loss = 0.05208375\n",
      "Iteration 173, loss = 0.04901933\n",
      "Iteration 174, loss = 0.04975058\n",
      "Iteration 175, loss = 0.04858899\n",
      "Iteration 176, loss = 0.04916947\n",
      "Iteration 177, loss = 0.04808473\n",
      "Iteration 178, loss = 0.04834259\n",
      "Iteration 179, loss = 0.04838755\n",
      "Iteration 180, loss = 0.04863161\n",
      "Iteration 181, loss = 0.04830817\n",
      "Iteration 182, loss = 0.04861117\n",
      "Iteration 183, loss = 0.04691521\n",
      "Iteration 184, loss = 0.04711646\n",
      "Iteration 185, loss = 0.04848564\n",
      "Iteration 186, loss = 0.04765669\n",
      "Iteration 187, loss = 0.04696207\n",
      "Iteration 188, loss = 0.04912829\n",
      "Iteration 189, loss = 0.04905912\n",
      "Iteration 190, loss = 0.04757264\n",
      "Iteration 191, loss = 0.04677172\n",
      "Iteration 192, loss = 0.04783686\n",
      "Iteration 193, loss = 0.04547873\n",
      "Iteration 194, loss = 0.04742151\n",
      "Iteration 195, loss = 0.04676287\n",
      "Iteration 196, loss = 0.04682778\n",
      "Iteration 197, loss = 0.04548541\n",
      "Iteration 198, loss = 0.04608959\n",
      "Iteration 199, loss = 0.04791182\n",
      "Iteration 200, loss = 0.04539936\n",
      "Iteration 201, loss = 0.04708290\n",
      "Iteration 202, loss = 0.04497676\n",
      "Iteration 203, loss = 0.04592448\n",
      "Iteration 204, loss = 0.04416279\n",
      "Iteration 205, loss = 0.04653102\n",
      "Iteration 206, loss = 0.04624900\n",
      "Iteration 207, loss = 0.04484877\n",
      "Iteration 208, loss = 0.04523098\n",
      "Iteration 209, loss = 0.04389074\n",
      "Iteration 210, loss = 0.04530456\n",
      "Iteration 211, loss = 0.04402609\n",
      "Iteration 212, loss = 0.04460046\n",
      "Iteration 213, loss = 0.04456872\n",
      "Iteration 214, loss = 0.04549399\n",
      "Iteration 215, loss = 0.04345264\n",
      "Iteration 216, loss = 0.04528275\n",
      "Iteration 217, loss = 0.04337656\n",
      "Iteration 218, loss = 0.04380813\n",
      "Iteration 219, loss = 0.04246293\n",
      "Iteration 220, loss = 0.04324385\n",
      "Iteration 221, loss = 0.04439950\n",
      "Iteration 222, loss = 0.04317158\n",
      "Iteration 223, loss = 0.04363741\n",
      "Iteration 224, loss = 0.04391832\n",
      "Iteration 225, loss = 0.04436644\n",
      "Iteration 226, loss = 0.04375195\n",
      "Iteration 227, loss = 0.04466412\n",
      "Iteration 228, loss = 0.04318101\n",
      "Iteration 229, loss = 0.04411081\n",
      "Iteration 230, loss = 0.04356223\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Algeria       0.22      0.23      0.22       359\n",
      "             Bahrain       0.00      0.00      0.00         8\n",
      "            Djibouti       0.00      0.00      0.00        10\n",
      "               Egypt       0.40      0.50      0.44      1070\n",
      "                Iraq       0.31      0.37      0.34       636\n",
      "              Jordan       0.02      0.01      0.01       104\n",
      "              Kuwait       0.03      0.04      0.04        70\n",
      "             Lebanon       0.06      0.07      0.07       110\n",
      "               Libya       0.08      0.06      0.07       265\n",
      "          Mauritania       0.08      0.10      0.09        40\n",
      "             Morocco       0.11      0.10      0.10       249\n",
      "                Oman       0.06      0.06      0.06       249\n",
      "           Palestine       0.06      0.04      0.05       102\n",
      "               Qatar       0.02      0.01      0.01       104\n",
      "        Saudi_Arabia       0.24      0.24      0.24       579\n",
      "             Somalia       0.00      0.00      0.00        51\n",
      "               Sudan       0.06      0.04      0.05        51\n",
      "               Syria       0.10      0.06      0.08       265\n",
      "             Tunisia       0.19      0.18      0.18       164\n",
      "United_Arab_Emirates       0.16      0.11      0.13       265\n",
      "               Yemen       0.10      0.09      0.09       206\n",
      "\n",
      "            accuracy                           0.23      4957\n",
      "           macro avg       0.11      0.11      0.11      4957\n",
      "        weighted avg       0.21      0.23      0.22      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train_original\n",
    "y_dev = y_dev_original\n",
    "\n",
    "clf = MLPClassifier((512,),verbose=True, max_iter=1000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f16b887afe49f1a3afd35971ffb1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362223d6c5394d91b9cd0903aa0f6a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fasttext-arabic\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "from pymagnitude import *\n",
    "\n",
    "\n",
    "glove = Magnitude(\"../downloads/fasttext-arabic/fasttext-arabic.magnitude\")\n",
    "def avg_glove(x):\n",
    "    vectors = []\n",
    "    for title in tqdm_notebook(x):\n",
    "        vectors.append(np.average(glove.query(word_tokenize(title)), axis = 0))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train = avg_glove(X_train_original)\n",
    "y_train = y_train_original\n",
    "X_dev = avg_glove(X_dev_original)\n",
    "y_dev = y_dev_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.54780982\n",
      "Iteration 2, loss = 2.26735667\n",
      "Iteration 3, loss = 2.15615065\n",
      "Iteration 4, loss = 2.09690508\n",
      "Iteration 5, loss = 2.05182844\n",
      "Iteration 6, loss = 2.01331439\n",
      "Iteration 7, loss = 1.97508404\n",
      "Iteration 8, loss = 1.94105449\n",
      "Iteration 9, loss = 1.90935690\n",
      "Iteration 10, loss = 1.87956038\n",
      "Iteration 11, loss = 1.84931800\n",
      "Iteration 12, loss = 1.81780445\n",
      "Iteration 13, loss = 1.78632814\n",
      "Iteration 14, loss = 1.75707251\n",
      "Iteration 15, loss = 1.72750847\n",
      "Iteration 16, loss = 1.69886592\n",
      "Iteration 17, loss = 1.66812514\n",
      "Iteration 18, loss = 1.63658566\n",
      "Iteration 19, loss = 1.60929224\n",
      "Iteration 20, loss = 1.57468367\n",
      "Iteration 21, loss = 1.54238460\n",
      "Iteration 22, loss = 1.51065198\n",
      "Iteration 23, loss = 1.47965689\n",
      "Iteration 24, loss = 1.44563343\n",
      "Iteration 25, loss = 1.41358176\n",
      "Iteration 26, loss = 1.37963780\n",
      "Iteration 27, loss = 1.34926548\n",
      "Iteration 28, loss = 1.31915753\n",
      "Iteration 29, loss = 1.28088079\n",
      "Iteration 30, loss = 1.25474591\n",
      "Iteration 31, loss = 1.21538587\n",
      "Iteration 32, loss = 1.18499822\n",
      "Iteration 33, loss = 1.14864445\n",
      "Iteration 34, loss = 1.11784596\n",
      "Iteration 35, loss = 1.08739062\n",
      "Iteration 36, loss = 1.04915054\n",
      "Iteration 37, loss = 1.01692699\n",
      "Iteration 38, loss = 0.98595046\n",
      "Iteration 39, loss = 0.94845961\n",
      "Iteration 40, loss = 0.92349805\n",
      "Iteration 41, loss = 0.88908414\n",
      "Iteration 42, loss = 0.86038859\n",
      "Iteration 43, loss = 0.82811693\n",
      "Iteration 44, loss = 0.80027006\n",
      "Iteration 45, loss = 0.76803025\n",
      "Iteration 46, loss = 0.74405682\n",
      "Iteration 47, loss = 0.71520239\n",
      "Iteration 48, loss = 0.68461554\n",
      "Iteration 49, loss = 0.66046851\n",
      "Iteration 50, loss = 0.62833014\n",
      "Iteration 51, loss = 0.60798134\n",
      "Iteration 52, loss = 0.58103935\n",
      "Iteration 53, loss = 0.55975432\n",
      "Iteration 54, loss = 0.53545929\n",
      "Iteration 55, loss = 0.51259229\n",
      "Iteration 56, loss = 0.49086369\n",
      "Iteration 57, loss = 0.46660514\n",
      "Iteration 58, loss = 0.44774930\n",
      "Iteration 59, loss = 0.42966039\n",
      "Iteration 60, loss = 0.40780800\n",
      "Iteration 61, loss = 0.39207169\n",
      "Iteration 62, loss = 0.37209425\n",
      "Iteration 63, loss = 0.35576805\n",
      "Iteration 64, loss = 0.34314983\n",
      "Iteration 65, loss = 0.32858904\n",
      "Iteration 66, loss = 0.30894962\n",
      "Iteration 67, loss = 0.29664274\n",
      "Iteration 68, loss = 0.28060215\n",
      "Iteration 69, loss = 0.26621580\n",
      "Iteration 70, loss = 0.26020154\n",
      "Iteration 71, loss = 0.24797548\n",
      "Iteration 72, loss = 0.23278335\n",
      "Iteration 73, loss = 0.22380024\n",
      "Iteration 74, loss = 0.21094940\n",
      "Iteration 75, loss = 0.20297747\n",
      "Iteration 76, loss = 0.19139238\n",
      "Iteration 77, loss = 0.18914351\n",
      "Iteration 78, loss = 0.17988626\n",
      "Iteration 79, loss = 0.16623761\n",
      "Iteration 80, loss = 0.15981670\n",
      "Iteration 81, loss = 0.15224770\n",
      "Iteration 82, loss = 0.14775758\n",
      "Iteration 83, loss = 0.14573975\n",
      "Iteration 84, loss = 0.14024878\n",
      "Iteration 85, loss = 0.13579496\n",
      "Iteration 86, loss = 0.12693208\n",
      "Iteration 87, loss = 0.12890820\n",
      "Iteration 88, loss = 0.12184232\n",
      "Iteration 89, loss = 0.11608949\n",
      "Iteration 90, loss = 0.11675549\n",
      "Iteration 91, loss = 0.11402988\n",
      "Iteration 92, loss = 0.11056346\n",
      "Iteration 93, loss = 0.10353992\n",
      "Iteration 94, loss = 0.10562084\n",
      "Iteration 95, loss = 0.09783300\n",
      "Iteration 96, loss = 0.09455495\n",
      "Iteration 97, loss = 0.09454325\n",
      "Iteration 98, loss = 0.09356645\n",
      "Iteration 99, loss = 0.09092663\n",
      "Iteration 100, loss = 0.09024604\n",
      "Iteration 101, loss = 0.09456050\n",
      "Iteration 102, loss = 0.08651718\n",
      "Iteration 103, loss = 0.08532554\n",
      "Iteration 104, loss = 0.08097256\n",
      "Iteration 105, loss = 0.08199424\n",
      "Iteration 106, loss = 0.08479969\n",
      "Iteration 107, loss = 0.07759276\n",
      "Iteration 108, loss = 0.07777759\n",
      "Iteration 109, loss = 0.08033330\n",
      "Iteration 110, loss = 0.08648768\n",
      "Iteration 111, loss = 0.08222084\n",
      "Iteration 112, loss = 0.08040787\n",
      "Iteration 113, loss = 0.07983394\n",
      "Iteration 114, loss = 0.07431624\n",
      "Iteration 115, loss = 0.07668030\n",
      "Iteration 116, loss = 0.07237893\n",
      "Iteration 117, loss = 0.07037100\n",
      "Iteration 118, loss = 0.07284307\n",
      "Iteration 119, loss = 0.07035681\n",
      "Iteration 120, loss = 0.07169436\n",
      "Iteration 121, loss = 0.06731459\n",
      "Iteration 122, loss = 0.06948026\n",
      "Iteration 123, loss = 0.06607898\n",
      "Iteration 124, loss = 0.06512634\n",
      "Iteration 125, loss = 0.06595460\n",
      "Iteration 126, loss = 0.07034815\n",
      "Iteration 127, loss = 0.06615680\n",
      "Iteration 128, loss = 0.06561674\n",
      "Iteration 129, loss = 0.06583855\n",
      "Iteration 130, loss = 0.06029671\n",
      "Iteration 131, loss = 0.06607819\n",
      "Iteration 132, loss = 0.06213206\n",
      "Iteration 133, loss = 0.06772934\n",
      "Iteration 134, loss = 0.06339814\n",
      "Iteration 135, loss = 0.06363832\n",
      "Iteration 136, loss = 0.06254367\n",
      "Iteration 137, loss = 0.06106086\n",
      "Iteration 138, loss = 0.05874463\n",
      "Iteration 139, loss = 0.06395091\n",
      "Iteration 140, loss = 0.06009107\n",
      "Iteration 141, loss = 0.05677793\n",
      "Iteration 142, loss = 0.06090377\n",
      "Iteration 143, loss = 0.05959784\n",
      "Iteration 144, loss = 0.05879187\n",
      "Iteration 145, loss = 0.05903991\n",
      "Iteration 146, loss = 0.06545341\n",
      "Iteration 147, loss = 0.05940602\n",
      "Iteration 148, loss = 0.05490904\n",
      "Iteration 149, loss = 0.06067975\n",
      "Iteration 150, loss = 0.05711627\n",
      "Iteration 151, loss = 0.05914379\n",
      "Iteration 152, loss = 0.05896938\n",
      "Iteration 153, loss = 0.05690922\n",
      "Iteration 154, loss = 0.05582116\n",
      "Iteration 155, loss = 0.05246083\n",
      "Iteration 156, loss = 0.05471879\n",
      "Iteration 157, loss = 0.05764196\n",
      "Iteration 158, loss = 0.05593557\n",
      "Iteration 159, loss = 0.05573485\n",
      "Iteration 160, loss = 0.06145956\n",
      "Iteration 161, loss = 0.05882387\n",
      "Iteration 162, loss = 0.05190281\n",
      "Iteration 163, loss = 0.05725470\n",
      "Iteration 164, loss = 0.05405613\n",
      "Iteration 165, loss = 0.05602270\n",
      "Iteration 166, loss = 0.04988287\n",
      "Iteration 167, loss = 0.04981170\n",
      "Iteration 168, loss = 0.05071640\n",
      "Iteration 169, loss = 0.05780557\n",
      "Iteration 170, loss = 0.05057487\n",
      "Iteration 171, loss = 0.05264708\n",
      "Iteration 172, loss = 0.05114614\n",
      "Iteration 173, loss = 0.05218753\n",
      "Iteration 174, loss = 0.05198047\n",
      "Iteration 175, loss = 0.05019762\n",
      "Iteration 176, loss = 0.05427326\n",
      "Iteration 177, loss = 0.05244596\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Algeria       0.35      0.37      0.36       359\n",
      "             Bahrain       0.00      0.00      0.00         8\n",
      "            Djibouti       0.04      0.10      0.06        10\n",
      "               Egypt       0.64      0.61      0.62      1070\n",
      "                Iraq       0.45      0.44      0.45       636\n",
      "              Jordan       0.13      0.10      0.11       104\n",
      "              Kuwait       0.06      0.14      0.09        70\n",
      "             Lebanon       0.15      0.13      0.14       110\n",
      "               Libya       0.23      0.29      0.26       265\n",
      "          Mauritania       0.08      0.10      0.09        40\n",
      "             Morocco       0.17      0.14      0.15       249\n",
      "                Oman       0.12      0.15      0.13       249\n",
      "           Palestine       0.13      0.11      0.12       102\n",
      "               Qatar       0.06      0.04      0.05       104\n",
      "        Saudi_Arabia       0.28      0.29      0.28       579\n",
      "             Somalia       0.00      0.00      0.00        51\n",
      "               Sudan       0.20      0.25      0.23        51\n",
      "               Syria       0.14      0.11      0.13       265\n",
      "             Tunisia       0.14      0.15      0.14       164\n",
      "United_Arab_Emirates       0.16      0.17      0.16       265\n",
      "               Yemen       0.23      0.19      0.21       206\n",
      "\n",
      "            accuracy                           0.32      4957\n",
      "           macro avg       0.18      0.18      0.18      4957\n",
      "        weighted avg       0.33      0.32      0.32      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train_original\n",
    "y_dev = y_dev_original\n",
    "\n",
    "clf = MLPClassifier((512,128),verbose=True, max_iter=1000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cc6edb064749b5a07bb5201b646551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48815b0cdf3d4a568af5b2eb84c3b34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1, loss = 2.66321309\n",
      "Iteration 2, loss = 2.55966751\n",
      "Iteration 3, loss = 2.50352172\n",
      "Iteration 4, loss = 2.46494889\n",
      "Iteration 5, loss = 2.43542133\n",
      "Iteration 6, loss = 2.41111855\n",
      "Iteration 7, loss = 2.38546963\n",
      "Iteration 8, loss = 2.36622255\n",
      "Iteration 9, loss = 2.34606205\n",
      "Iteration 10, loss = 2.32784453\n",
      "Iteration 11, loss = 2.30855527\n",
      "Iteration 12, loss = 2.28811847\n",
      "Iteration 13, loss = 2.26796048\n",
      "Iteration 14, loss = 2.24949202\n",
      "Iteration 15, loss = 2.22805270\n",
      "Iteration 16, loss = 2.20353293\n",
      "Iteration 17, loss = 2.18381858\n",
      "Iteration 18, loss = 2.15625919\n",
      "Iteration 19, loss = 2.13572188\n",
      "Iteration 20, loss = 2.11436456\n",
      "Iteration 21, loss = 2.08350234\n",
      "Iteration 22, loss = 2.06645489\n",
      "Iteration 23, loss = 2.03372013\n",
      "Iteration 24, loss = 2.00547214\n",
      "Iteration 25, loss = 1.98385747\n",
      "Iteration 26, loss = 1.95369671\n",
      "Iteration 27, loss = 1.93038488\n",
      "Iteration 28, loss = 1.89821789\n",
      "Iteration 29, loss = 1.87506110\n",
      "Iteration 30, loss = 1.84792540\n",
      "Iteration 31, loss = 1.81537623\n",
      "Iteration 32, loss = 1.79472290\n",
      "Iteration 33, loss = 1.76458215\n",
      "Iteration 34, loss = 1.73059083\n",
      "Iteration 35, loss = 1.70330401\n",
      "Iteration 36, loss = 1.67904571\n",
      "Iteration 37, loss = 1.64658331\n",
      "Iteration 38, loss = 1.61648429\n",
      "Iteration 39, loss = 1.58950712\n",
      "Iteration 40, loss = 1.56201824\n",
      "Iteration 41, loss = 1.53344671\n",
      "Iteration 42, loss = 1.51200477\n",
      "Iteration 43, loss = 1.48893424\n",
      "Iteration 44, loss = 1.45112594\n",
      "Iteration 45, loss = 1.41970816\n",
      "Iteration 46, loss = 1.39107188\n",
      "Iteration 47, loss = 1.36980951\n",
      "Iteration 48, loss = 1.34144411\n",
      "Iteration 49, loss = 1.32263790\n",
      "Iteration 50, loss = 1.29771198\n",
      "Iteration 51, loss = 1.26051221\n",
      "Iteration 52, loss = 1.23334978\n",
      "Iteration 53, loss = 1.20688261\n",
      "Iteration 54, loss = 1.18554394\n",
      "Iteration 55, loss = 1.16780778\n",
      "Iteration 56, loss = 1.13056313\n",
      "Iteration 57, loss = 1.10576053\n",
      "Iteration 58, loss = 1.09396876\n",
      "Iteration 59, loss = 1.05990527\n",
      "Iteration 60, loss = 1.03821833\n",
      "Iteration 61, loss = 1.01354349\n",
      "Iteration 62, loss = 0.99368373\n",
      "Iteration 63, loss = 0.96754760\n",
      "Iteration 64, loss = 0.94262200\n",
      "Iteration 65, loss = 0.93176760\n",
      "Iteration 66, loss = 0.90148523\n",
      "Iteration 67, loss = 0.87766927\n",
      "Iteration 68, loss = 0.86563986\n",
      "Iteration 69, loss = 0.84956897\n",
      "Iteration 70, loss = 0.81879971\n",
      "Iteration 71, loss = 0.79992587\n",
      "Iteration 72, loss = 0.77739528\n",
      "Iteration 73, loss = 0.76838173\n",
      "Iteration 74, loss = 0.73535690\n",
      "Iteration 75, loss = 0.72311538\n",
      "Iteration 76, loss = 0.69979170\n",
      "Iteration 77, loss = 0.68741798\n",
      "Iteration 78, loss = 0.66758884\n",
      "Iteration 79, loss = 0.64406866\n",
      "Iteration 80, loss = 0.63240249\n",
      "Iteration 81, loss = 0.62545255\n",
      "Iteration 82, loss = 0.60680108\n",
      "Iteration 83, loss = 0.58910364\n",
      "Iteration 84, loss = 0.57225643\n",
      "Iteration 85, loss = 0.55062251\n",
      "Iteration 86, loss = 0.53512889\n",
      "Iteration 87, loss = 0.53209322\n",
      "Iteration 88, loss = 0.50703911\n",
      "Iteration 89, loss = 0.50299403\n",
      "Iteration 90, loss = 0.48748336\n",
      "Iteration 91, loss = 0.47405361\n",
      "Iteration 92, loss = 0.45390079\n",
      "Iteration 93, loss = 0.44430538\n",
      "Iteration 94, loss = 0.42989862\n",
      "Iteration 95, loss = 0.42744758\n",
      "Iteration 96, loss = 0.40891879\n",
      "Iteration 97, loss = 0.39999012\n",
      "Iteration 98, loss = 0.39092055\n",
      "Iteration 99, loss = 0.37460556\n",
      "Iteration 100, loss = 0.36760483\n",
      "Iteration 101, loss = 0.35031663\n",
      "Iteration 102, loss = 0.34631892\n",
      "Iteration 103, loss = 0.33449679\n",
      "Iteration 104, loss = 0.32736308\n",
      "Iteration 105, loss = 0.31644327\n",
      "Iteration 106, loss = 0.30944890\n",
      "Iteration 107, loss = 0.29788669\n",
      "Iteration 108, loss = 0.29593299\n",
      "Iteration 109, loss = 0.28286455\n",
      "Iteration 110, loss = 0.27819936\n",
      "Iteration 111, loss = 0.26318728\n",
      "Iteration 112, loss = 0.24917487\n",
      "Iteration 113, loss = 0.24957447\n",
      "Iteration 114, loss = 0.24045358\n",
      "Iteration 115, loss = 0.23014132\n",
      "Iteration 116, loss = 0.23247540\n",
      "Iteration 117, loss = 0.23439164\n",
      "Iteration 118, loss = 0.24691534\n",
      "Iteration 119, loss = 0.20836837\n",
      "Iteration 120, loss = 0.20556751\n",
      "Iteration 121, loss = 0.20188706\n",
      "Iteration 122, loss = 0.18510833\n",
      "Iteration 123, loss = 0.18496473\n",
      "Iteration 124, loss = 0.18714994\n",
      "Iteration 125, loss = 0.18710413\n",
      "Iteration 126, loss = 0.18037624\n",
      "Iteration 127, loss = 0.17333233\n",
      "Iteration 128, loss = 0.16596671\n",
      "Iteration 129, loss = 0.16256213\n",
      "Iteration 130, loss = 0.15298225\n",
      "Iteration 131, loss = 0.15637727\n",
      "Iteration 132, loss = 0.14471698\n",
      "Iteration 133, loss = 0.15618199\n",
      "Iteration 134, loss = 0.17361074\n",
      "Iteration 135, loss = 0.13975500\n",
      "Iteration 136, loss = 0.15185907\n",
      "Iteration 137, loss = 0.14397040\n",
      "Iteration 138, loss = 0.13448725\n",
      "Iteration 139, loss = 0.13192737\n",
      "Iteration 140, loss = 0.13704615\n",
      "Iteration 141, loss = 0.13186096\n",
      "Iteration 142, loss = 0.11918536\n",
      "Iteration 143, loss = 0.13285157\n",
      "Iteration 144, loss = 0.12668367\n",
      "Iteration 145, loss = 0.13399551\n",
      "Iteration 146, loss = 0.11321202\n",
      "Iteration 147, loss = 0.11173624\n",
      "Iteration 148, loss = 0.10835843\n",
      "Iteration 149, loss = 0.11479137\n",
      "Iteration 150, loss = 0.12512905\n",
      "Iteration 151, loss = 0.12299498\n",
      "Iteration 152, loss = 0.11964611\n",
      "Iteration 153, loss = 0.11026417\n",
      "Iteration 154, loss = 0.09461576\n",
      "Iteration 155, loss = 0.10550811\n",
      "Iteration 156, loss = 0.09446153\n",
      "Iteration 157, loss = 0.11054885\n",
      "Iteration 158, loss = 0.09974452\n",
      "Iteration 159, loss = 0.09891920\n",
      "Iteration 160, loss = 0.09093857\n",
      "Iteration 161, loss = 0.09313290\n",
      "Iteration 162, loss = 0.10425037\n",
      "Iteration 163, loss = 0.09697349\n",
      "Iteration 164, loss = 0.09361731\n",
      "Iteration 165, loss = 0.09955369\n",
      "Iteration 166, loss = 0.08674140\n",
      "Iteration 167, loss = 0.08810008\n",
      "Iteration 168, loss = 0.10344513\n",
      "Iteration 169, loss = 0.10069282\n",
      "Iteration 170, loss = 0.09445138\n",
      "Iteration 171, loss = 0.09023984\n",
      "Iteration 172, loss = 0.08427429\n",
      "Iteration 173, loss = 0.08474705\n",
      "Iteration 174, loss = 0.08307785\n",
      "Iteration 175, loss = 0.08974476\n",
      "Iteration 176, loss = 0.08637252\n",
      "Iteration 177, loss = 0.08980920\n",
      "Iteration 178, loss = 0.09134028\n",
      "Iteration 179, loss = 0.07906198\n",
      "Iteration 180, loss = 0.07876396\n",
      "Iteration 181, loss = 0.07380696\n",
      "Iteration 182, loss = 0.08031662\n",
      "Iteration 183, loss = 0.09129808\n",
      "Iteration 184, loss = 0.07742221\n",
      "Iteration 185, loss = 0.08073301\n",
      "Iteration 186, loss = 0.07680258\n",
      "Iteration 187, loss = 0.08778599\n",
      "Iteration 188, loss = 0.08324818\n",
      "Iteration 189, loss = 0.08288894\n",
      "Iteration 190, loss = 0.09138392\n",
      "Iteration 191, loss = 0.10233770\n",
      "Iteration 192, loss = 0.08739879\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Algeria       0.16      0.26      0.20       359\n",
      "             Bahrain       0.00      0.00      0.00         8\n",
      "            Djibouti       0.00      0.00      0.00        10\n",
      "               Egypt       0.47      0.44      0.45      1070\n",
      "                Iraq       0.30      0.32      0.31       636\n",
      "              Jordan       0.01      0.01      0.01       104\n",
      "              Kuwait       0.04      0.04      0.04        70\n",
      "             Lebanon       0.02      0.02      0.02       110\n",
      "               Libya       0.12      0.09      0.10       265\n",
      "          Mauritania       0.04      0.05      0.04        40\n",
      "             Morocco       0.05      0.06      0.06       249\n",
      "                Oman       0.09      0.07      0.08       249\n",
      "           Palestine       0.02      0.02      0.02       102\n",
      "               Qatar       0.02      0.01      0.01       104\n",
      "        Saudi_Arabia       0.20      0.14      0.17       579\n",
      "             Somalia       0.08      0.02      0.03        51\n",
      "               Sudan       0.03      0.02      0.02        51\n",
      "               Syria       0.08      0.09      0.09       265\n",
      "             Tunisia       0.08      0.10      0.09       164\n",
      "United_Arab_Emirates       0.12      0.10      0.11       265\n",
      "               Yemen       0.07      0.08      0.08       206\n",
      "\n",
      "            accuracy                           0.20      4957\n",
      "           macro avg       0.09      0.09      0.09      4957\n",
      "        weighted avg       0.21      0.20      0.20      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#glove-english\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "from pymagnitude import *\n",
    "\n",
    "\n",
    "glove = Magnitude(\"../../speech/downloads/glove.6B.300d.magnitude\")\n",
    "def avg_glove(x):\n",
    "    vectors = []\n",
    "    for title in tqdm_notebook(x):\n",
    "        vectors.append(np.average(glove.query(word_tokenize(title)), axis = 0))\n",
    "    return np.array(vectors)\n",
    "\n",
    "X_train = avg_glove(X_train_original)\n",
    "y_train = y_train_original\n",
    "X_dev = avg_glove(X_dev_original)\n",
    "y_dev = y_dev_original\n",
    "\n",
    "y_train = y_train_original\n",
    "y_dev = y_dev_original\n",
    "\n",
    "clf = MLPClassifier((512,128),verbose=True, max_iter=1000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
