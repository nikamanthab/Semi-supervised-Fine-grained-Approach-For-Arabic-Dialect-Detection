{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikamanth/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.utils import to_categorical\n",
    "import gensim\n",
    "import numpy as np\n",
    "from keras.callbacks import callbacks\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(file_name, preds):\n",
    "    with open('../results/'+file_name, 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    with open('../results/bert_result.txt', 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    !python ../NADI-2020_release_1.0/NADI_release/NADI-DID-Scorer.py ../tsv/gold1.txt ../results/bert_result.txt\n",
    "\n",
    "def write_result2(file_name, preds):\n",
    "    with open('../results/'+file_name, 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    with open('../results/bert_result.txt', 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    !python ../NADI-2020_release_1.0/NADI_release/NADI-DID-Scorer.py ../tsv/gold2.txt ../results/bert_result.txt\n",
    "\n",
    "def preprocess_text(train_list, test_list):\n",
    "    X_train_corrected_tweets = []\n",
    "    for tweet in tqdm(train_list):\n",
    "        new_tweet = re.findall( '[^A-Za-z:/_.0-9\\\\#@,=+\\(\\)]+' ,tweet)\n",
    "        new_tweet = \" \".join(new_tweet).replace('\\xa0','').replace('\\u200c','').replace('\\U000fe329','').replace('\\u2066','').replace('\\u2069','').strip()\n",
    "        X_train_corrected_tweets.append(new_tweet)\n",
    "\n",
    "    X_dev_corrected_tweets = []\n",
    "    for tweet in tqdm(test_list):\n",
    "        new_tweet = re.findall( '[^A-Za-z:/_.0-9\\\\#@,=+\\(\\)]+' ,tweet) #[^\\x00-\\x19\\x21-\\x7F]+\n",
    "        new_tweet = \" \".join(new_tweet).replace('\\xa0','').replace('\\u200c','').replace('\\U000fe329','').replace('\\u2066','').replace('\\u2069','').strip()\n",
    "        X_dev_corrected_tweets.append(new_tweet)\n",
    "    return X_train_corrected_tweets, X_dev_corrected_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert - multilingual\n",
    "bert = SentenceTransformer('distiluse-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#3 country_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#3 country_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=21)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e420e80429246c9b46984c0db295b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dff1e77c97d43de820706d1bc71e765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 657/657 [04:05<00:00,  2.67it/s]\n",
      "Batches: 100%|██████████| 155/155 [00:57<00:00,  2.71it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_corrected, X_dev_corrected = preprocess_text(X_train_original, X_dev_original)\n",
    "X_train = bert.encode(X_train_corrected,batch_size=32,show_progress_bar=True)\n",
    "X_dev = bert.encode(X_dev_corrected,batch_size=32,show_progress_bar=True)\n",
    "X_train = np.vstack(X_train)\n",
    "X_dev = np.vstack(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "21000/21000 [==============================] - 2s 115us/step - loss: 2.5219 - accuracy: 0.2566 - val_loss: 2.4410 - val_accuracy: 0.2855\n",
      "Epoch 2/50\n",
      "21000/21000 [==============================] - 2s 109us/step - loss: 2.3499 - accuracy: 0.3030 - val_loss: 2.4284 - val_accuracy: 0.2957\n",
      "Epoch 3/50\n",
      "21000/21000 [==============================] - 2s 109us/step - loss: 2.2623 - accuracy: 0.3258 - val_loss: 2.4256 - val_accuracy: 0.2951\n",
      "Epoch 4/50\n",
      "21000/21000 [==============================] - 2s 109us/step - loss: 2.1835 - accuracy: 0.3427 - val_loss: 2.4552 - val_accuracy: 0.2855\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(21, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=1, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Iraq', 1: 'Egypt', 2: 'Morocco', 3: 'Libya', 4: 'United_Arab_Emirates', 5: 'Mauritania', 6: 'Saudi_Arabia', 7: 'Bahrain', 8: 'Syria', 9: 'Djibouti', 10: 'Lebanon', 11: 'Oman', 12: 'Palestine', 13: 'Algeria', 14: 'Somalia', 15: 'Jordan', 16: 'Tunisia', 17: 'Kuwait', 18: 'Yemen', 19: 'Sudan', 20: 'Qatar'}\n",
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 15.56 %\n",
      "MACRO AVERAGE RECALL SCORE: 11.60 %\n",
      "MACRO AVERAGE F1 SCORE: 11.13 %\n",
      "OVERALL ACCURACY: 29.51 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result('bert_task1_unbal.txt',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../tsv/oversample_train.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#3 country_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#3 country_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=21)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88027a2d299641fcbe5fc8a6c6df5e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=93933.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e934dcb44cf24439baf57a90d62223de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 2/2936 [00:00<03:16, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2936/2936 [18:40<00:00,  2.62it/s]\n",
      "Batches: 100%|██████████| 155/155 [00:58<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_corrected, X_dev_corrected = preprocess_text(X_train_original, X_dev_original)\n",
    "X_train = bert.encode(X_train_corrected,batch_size=32,show_progress_bar=True)\n",
    "X_dev = bert.encode(X_dev_corrected,batch_size=32,show_progress_bar=True)\n",
    "X_train = np.vstack(X_train)\n",
    "X_dev = np.vstack(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 93933 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "93933/93933 [==============================] - 10s 109us/step - loss: 2.2775 - accuracy: 0.3423 - val_loss: 2.8395 - val_accuracy: 0.1455\n",
      "Epoch 2/50\n",
      "93933/93933 [==============================] - 10s 105us/step - loss: 1.5558 - accuracy: 0.5585 - val_loss: 2.9117 - val_accuracy: 0.1582\n",
      "Epoch 3/50\n",
      "93933/93933 [==============================] - 10s 106us/step - loss: 1.1250 - accuracy: 0.6880 - val_loss: 3.0205 - val_accuracy: 0.1810\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(21, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=2, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Lebanon', 1: 'Sudan', 2: 'Djibouti', 3: 'Saudi_Arabia', 4: 'Oman', 5: 'Yemen', 6: 'Bahrain', 7: 'Mauritania', 8: 'Kuwait', 9: 'Iraq', 10: 'Palestine', 11: 'Jordan', 12: 'Somalia', 13: 'Tunisia', 14: 'Morocco', 15: 'Syria', 16: 'United_Arab_Emirates', 17: 'Algeria', 18: 'Egypt', 19: 'Qatar', 20: 'Libya'}\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 13.40 %\n",
      "MACRO AVERAGE RECALL SCORE: 11.00 %\n",
      "MACRO AVERAGE F1 SCORE: 9.85 %\n",
      "OVERALL ACCURACY: 14.55 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result('bert_task1_bal.txt',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c4d409193d406489f9e4b08df9496c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv','\\t')\n",
    "g = train_df['#4 province_label'].value_counts()[0]\n",
    "for i in tqdm_notebook(train_df['#4 province_label'].unique()):\n",
    "    filter_df = train_df[train_df['#4 province_label'] == i]\n",
    "    count = len(filter_df)\n",
    "    while (count<=g):\n",
    "        count+=1\n",
    "        idx = random.randint(0,len(filter_df)-1)\n",
    "        train_df = pd.concat([train_df,filter_df[idx:idx+1]])\n",
    "train_df['#4 province_label'].value_counts()\n",
    "train_df.to_csv('../tsv/oversample_train2.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#4 province_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#4 province_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=100)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4feb5dc7124a9c8792eda0f39a9bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cff1b67e1348b0a9f58a980ee10adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 657/657 [04:07<00:00,  2.66it/s]\n",
      "Batches: 100%|██████████| 155/155 [00:57<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_corrected, X_dev_corrected = preprocess_text(X_train_original, X_dev_original)\n",
    "X_train = bert.encode(X_train_corrected,batch_size=32,show_progress_bar=True)\n",
    "X_dev = bert.encode(X_dev_corrected,batch_size=32,show_progress_bar=True)\n",
    "X_train = np.vstack(X_train)\n",
    "X_dev = np.vstack(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "21000/21000 [==============================] - 3s 143us/step - loss: 4.4520 - accuracy: 0.0488 - val_loss: 4.4893 - val_accuracy: 0.0317\n",
      "Epoch 2/50\n",
      "21000/21000 [==============================] - 3s 123us/step - loss: 4.1630 - accuracy: 0.0902 - val_loss: 4.4787 - val_accuracy: 0.0353\n",
      "Epoch 3/50\n",
      "21000/21000 [==============================] - 3s 125us/step - loss: 3.9883 - accuracy: 0.1178 - val_loss: 4.4951 - val_accuracy: 0.0363\n",
      "Epoch 4/50\n",
      "21000/21000 [==============================] - 3s 135us/step - loss: 3.8439 - accuracy: 0.1409 - val_loss: 4.5380 - val_accuracy: 0.0369\n",
      "Epoch 5/50\n",
      "21000/21000 [==============================] - 3s 132us/step - loss: 3.7154 - accuracy: 0.1609 - val_loss: 4.5986 - val_accuracy: 0.0341\n",
      "Epoch 6/50\n",
      "21000/21000 [==============================] - 3s 128us/step - loss: 3.5923 - accuracy: 0.1830 - val_loss: 4.6482 - val_accuracy: 0.0359\n",
      "Epoch 7/50\n",
      "21000/21000 [==============================] - 3s 128us/step - loss: 3.4785 - accuracy: 0.2020 - val_loss: 4.7037 - val_accuracy: 0.0361\n",
      "Epoch 8/50\n",
      "21000/21000 [==============================] - 3s 129us/step - loss: 3.3638 - accuracy: 0.2250 - val_loss: 4.7476 - val_accuracy: 0.0339\n",
      "Epoch 9/50\n",
      "21000/21000 [==============================] - 3s 130us/step - loss: 3.2552 - accuracy: 0.2460 - val_loss: 4.8023 - val_accuracy: 0.0385\n",
      "Epoch 10/50\n",
      "21000/21000 [==============================] - 3s 128us/step - loss: 3.1449 - accuracy: 0.2666 - val_loss: 4.8638 - val_accuracy: 0.0357\n",
      "Epoch 11/50\n",
      "21000/21000 [==============================] - 3s 130us/step - loss: 3.0392 - accuracy: 0.2889 - val_loss: 4.9153 - val_accuracy: 0.0379\n",
      "Epoch 12/50\n",
      "21000/21000 [==============================] - 3s 130us/step - loss: 2.9315 - accuracy: 0.3141 - val_loss: 4.9764 - val_accuracy: 0.0379\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=10, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'iq_Al-Anbar', 1: 'eg_Alexandria', 2: 'iq_Maysan', 3: 'ma_Oriental', 4: 'ly_Al-Jabal-al-Akhdar', 5: 'ae_Fujairah', 6: 'eg_Ismailia', 7: 'iq_Baghdad', 8: 'eg_Dakahlia', 9: 'mr_Nouakchott', 10: 'eg_Qena', 11: 'ma_Marrakech-Tensift-Al-Haouz', 12: 'sa_Tabuk', 13: 'eg_Asyut', 14: 'iq_Karbala', 15: 'bh_Capital', 16: 'sy_Damascus-City', 17: 'sa_Najran', 18: 'dj_Djibouti', 19: 'lb_Akkar', 20: 'om_Musandam', 21: 'ps_Gaza-Strip', 22: 'dz_Oran', 23: 'so_Banaadir', 24: 'sy_As-Suwayda', 25: 'eg_Faiyum', 26: 'jo_Aqaba', 27: 'eg_Cairo', 28: 'lb_North-Lebanon', 29: 'eg_Port-Said', 30: 'eg_Monufia', 31: 'tn_Sousse', 32: 'eg_Beheira', 33: 'sa_Ash-Sharqiyah', 34: 'eg_Gharbia', 35: 'ae_Ras-Al-Khaymah', 36: 'eg_Minya', 37: 'om_Al-Batnah', 38: 'kw_Jahra', 39: 'dz_Jijel', 40: 'dz_Béchar', 41: 'eg_Sohag', 42: 'sy_Hims', 43: 'iq_An-Najaf', 44: 'lb_South-Lebanon', 45: 'eg_Aswan', 46: 'ye_Ibb', 47: 'iq_Dihok', 48: 'sy_Aleppo', 49: 'iq_Al-Muthannia', 50: 'dz_Bordj-Bou-Arreridj\\u200e', 51: 'ly_Tripoli', 52: 'ye_Dhamar', 53: 'ly_Misrata', 54: 'tn_Ariana', 55: 'ma_Meknes-Tafilalet', 56: 'iq_Wasit', 57: \"sa_Ha'il\", 58: 'sa_Jizan', 59: 'sa_Al-Quassim', 60: 'om_Muscat', 61: 'ly_Al-Butnan', 62: 'eg_Beni-Suef', 63: 'ae_Dubai', 64: 'eg_Suez', 65: 'ly_Benghazi', 66: 'eg_Red-Sea', 67: 'eg_North-Sinai', 68: 'sd_Khartoum', 69: 'eg_South-Sinai', 70: 'tn_Mahdia', 71: 'ma_Tanger-Tetouan', 72: 'ae_Abu-Dhabi', 73: 'om_Dhofar', 74: 'iq_As-Sulaymaniyah', 75: 'iq_Basra', 76: 'om_Ad-Dakhiliyah', 77: 'dz_Khenchela', 78: 'jo_Zarqa', 79: 'iq_Arbil', 80: 'sa_Al-Madinah', 81: 'eg_Kafr-el-Sheikh', 82: 'sa_Ar-Riyad', 83: 'kw_Hawalli', 84: 'dz_Ouargla', 85: 'ye_Al-Hudaydah', 86: 'tn_Kairouan', 87: 'ps_West-Bank', 88: 'qa_Ar-Rayyan', 89: 'sa_Asir', 90: 'ma_Souss-Massa-Draa', 91: 'iq_Ninawa', 92: 'eg_Luxor', 93: 'sy_Lattakia', 94: 'sa_Makkah', 95: 'om_Ash-Sharqiyah', 96: 'ye_Aden', 97: 'dz_Bouira', 98: 'ae_Umm-Al-Qaywayn', 99: 'qa_Doha'}\n",
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 3.17 %\n",
      "MACRO AVERAGE RECALL SCORE: 4.16 %\n",
      "MACRO AVERAGE F1 SCORE: 2.59 %\n",
      "OVERALL ACCURACY: 3.53 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result2('bert_task2_unbal.txt',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../tsv/oversample_train2.tsv')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#4 province_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#4 province_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "uni = y_train_original.unique()\n",
    "for i in range(len(uni)):\n",
    "    labels[uni[i]] = i\n",
    "\n",
    "y_train_index = [labels[i] for i in y_train_original]\n",
    "y_dev_index = [labels[i] for i in y_dev_original]\n",
    "\n",
    "y_train = to_categorical(y_train_index, num_classes=100)\n",
    "y_dev = to_categorical(y_dev_index, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf8feb5b2dd42b9b6dbb066acb9040b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d556a136f7449c2892274d9f24c0eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Batches:   0%|          | 0/1238 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1238/1238 [08:15<00:00,  2.50it/s]\n",
      "Batches: 100%|██████████| 155/155 [01:01<00:00,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_corrected, X_dev_corrected = preprocess_text(X_train_original, X_dev_original)\n",
    "X_train = bert.encode(X_train_corrected,batch_size=32,show_progress_bar=True)\n",
    "X_dev = bert.encode(X_dev_corrected,batch_size=32,show_progress_bar=True)\n",
    "X_train = np.vstack(X_train)\n",
    "X_dev = np.vstack(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39600 samples, validate on 4957 samples\n",
      "Epoch 1/50\n",
      "39600/39600 [==============================] - 5s 125us/step - loss: 4.2835 - accuracy: 0.0838 - val_loss: 4.5187 - val_accuracy: 0.0297\n",
      "Epoch 2/50\n",
      "39600/39600 [==============================] - 5s 119us/step - loss: 3.8098 - accuracy: 0.1594 - val_loss: 4.6084 - val_accuracy: 0.0311\n",
      "Epoch 3/50\n",
      "39600/39600 [==============================] - 5s 119us/step - loss: 3.5040 - accuracy: 0.2179 - val_loss: 4.7482 - val_accuracy: 0.0323\n",
      "Epoch 4/50\n",
      "39600/39600 [==============================] - 5s 121us/step - loss: 3.2453 - accuracy: 0.2734 - val_loss: 4.8741 - val_accuracy: 0.0341\n",
      "Epoch 5/50\n",
      "39600/39600 [==============================] - 5s 120us/step - loss: 3.0085 - accuracy: 0.3277 - val_loss: 5.0202 - val_accuracy: 0.0331\n",
      "Epoch 6/50\n",
      "39600/39600 [==============================] - 5s 122us/step - loss: 2.7892 - accuracy: 0.3763 - val_loss: 5.1588 - val_accuracy: 0.0307\n",
      "Epoch 7/50\n",
      "39600/39600 [==============================] - 5s 127us/step - loss: 2.5804 - accuracy: 0.4316 - val_loss: 5.2843 - val_accuracy: 0.0349\n",
      "Epoch 8/50\n",
      "39600/39600 [==============================] - 5s 128us/step - loss: 2.3824 - accuracy: 0.4796 - val_loss: 5.4256 - val_accuracy: 0.0335\n",
      "Epoch 9/50\n",
      "39600/39600 [==============================] - 5s 128us/step - loss: 2.1967 - accuracy: 0.5277 - val_loss: 5.5673 - val_accuracy: 0.0339\n",
      "Epoch 10/50\n",
      "39600/39600 [==============================] - 5s 132us/step - loss: 2.0193 - accuracy: 0.5724 - val_loss: 5.7133 - val_accuracy: 0.0323\n",
      "Epoch 11/50\n",
      "39600/39600 [==============================] - 5s 125us/step - loss: 1.8539 - accuracy: 0.6148 - val_loss: 5.8663 - val_accuracy: 0.0333\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=50,validation_data=(X_dev,y_dev),\n",
    "                   callbacks=[callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                                      patience=10, verbose=0, mode='auto', \n",
    "                                                      baseline=None, restore_best_weights=True)])\n",
    "prediction = model.predict_classes(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'iq_Al-Anbar', 1: 'eg_Alexandria', 2: 'iq_Maysan', 3: 'ma_Oriental', 4: 'ly_Al-Jabal-al-Akhdar', 5: 'ae_Fujairah', 6: 'eg_Ismailia', 7: 'iq_Baghdad', 8: 'eg_Dakahlia', 9: 'mr_Nouakchott', 10: 'eg_Qena', 11: 'ma_Marrakech-Tensift-Al-Haouz', 12: 'sa_Tabuk', 13: 'eg_Asyut', 14: 'iq_Karbala', 15: 'bh_Capital', 16: 'sy_Damascus-City', 17: 'sa_Najran', 18: 'dj_Djibouti', 19: 'lb_Akkar', 20: 'om_Musandam', 21: 'ps_Gaza-Strip', 22: 'dz_Oran', 23: 'so_Banaadir', 24: 'sy_As-Suwayda', 25: 'eg_Faiyum', 26: 'jo_Aqaba', 27: 'eg_Cairo', 28: 'lb_North-Lebanon', 29: 'eg_Port-Said', 30: 'eg_Monufia', 31: 'tn_Sousse', 32: 'eg_Beheira', 33: 'sa_Ash-Sharqiyah', 34: 'eg_Gharbia', 35: 'ae_Ras-Al-Khaymah', 36: 'eg_Minya', 37: 'om_Al-Batnah', 38: 'kw_Jahra', 39: 'dz_Jijel', 40: 'dz_Béchar', 41: 'eg_Sohag', 42: 'sy_Hims', 43: 'iq_An-Najaf', 44: 'lb_South-Lebanon', 45: 'eg_Aswan', 46: 'ye_Ibb', 47: 'iq_Dihok', 48: 'sy_Aleppo', 49: 'iq_Al-Muthannia', 50: 'dz_Bordj-Bou-Arreridj\\u200e', 51: 'ly_Tripoli', 52: 'ye_Dhamar', 53: 'ly_Misrata', 54: 'tn_Ariana', 55: 'ma_Meknes-Tafilalet', 56: 'iq_Wasit', 57: \"sa_Ha'il\", 58: 'sa_Jizan', 59: 'sa_Al-Quassim', 60: 'om_Muscat', 61: 'ly_Al-Butnan', 62: 'eg_Beni-Suef', 63: 'ae_Dubai', 64: 'eg_Suez', 65: 'ly_Benghazi', 66: 'eg_Red-Sea', 67: 'eg_North-Sinai', 68: 'sd_Khartoum', 69: 'eg_South-Sinai', 70: 'tn_Mahdia', 71: 'ma_Tanger-Tetouan', 72: 'ae_Abu-Dhabi', 73: 'om_Dhofar', 74: 'iq_As-Sulaymaniyah', 75: 'iq_Basra', 76: 'om_Ad-Dakhiliyah', 77: 'dz_Khenchela', 78: 'jo_Zarqa', 79: 'iq_Arbil', 80: 'sa_Al-Madinah', 81: 'eg_Kafr-el-Sheikh', 82: 'sa_Ar-Riyad', 83: 'kw_Hawalli', 84: 'dz_Ouargla', 85: 'ye_Al-Hudaydah', 86: 'tn_Kairouan', 87: 'ps_West-Bank', 88: 'qa_Ar-Rayyan', 89: 'sa_Asir', 90: 'ma_Souss-Massa-Draa', 91: 'iq_Ninawa', 92: 'eg_Luxor', 93: 'sy_Lattakia', 94: 'sa_Makkah', 95: 'om_Ash-Sharqiyah', 96: 'ye_Aden', 97: 'dz_Bouira', 98: 'ae_Umm-Al-Qaywayn', 99: 'qa_Doha'}\n",
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 2.31 %\n",
      "MACRO AVERAGE RECALL SCORE: 2.97 %\n",
      "MACRO AVERAGE F1 SCORE: 1.97 %\n",
      "OVERALL ACCURACY: 2.97 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reverse_label_map = {value : key for (key, value) in labels.items()}\n",
    "print(reverse_label_map)\n",
    "pred = model.predict_classes(X_dev)\n",
    "pred = [reverse_label_map[i] for i in pred]\n",
    "# print(pred)\n",
    "len(pred)\n",
    "write_result2('bert_task2_unbal.txt',pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
