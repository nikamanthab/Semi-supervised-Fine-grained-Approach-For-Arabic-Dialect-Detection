{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(file_name, preds):\n",
    "    with open('../results/'+file_name, 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    with open('../results/last_result.txt', 'wt') as file:\n",
    "        for i in preds:\n",
    "            file.write(i+'\\n')\n",
    "    !python ../NADI-2020_release_1.0/NADI_release/NADI-DID-Scorer.py ../tsv/gold1.txt ../results/last_result.txt\n",
    "\n",
    "def preprocess_text(train_list, test_list):\n",
    "    X_train_corrected_tweets = []\n",
    "    for tweet in tqdm(train_list):\n",
    "        new_tweet = re.findall( '[^A-Za-z:/_.0-9\\\\#@,=+\\(\\)]+' ,tweet)\n",
    "        new_tweet = \" \".join(new_tweet).replace('\\xa0','').replace('\\u200c','').replace('\\U000fe329','').replace('\\u2066','').replace('\\u2069','').strip()\n",
    "        X_train_corrected_tweets.append(new_tweet)\n",
    "\n",
    "    X_dev_corrected_tweets = []\n",
    "    for tweet in tqdm(test_list):\n",
    "        new_tweet = re.findall( '[^A-Za-z:/_.0-9\\\\#@,=+\\(\\)]+' ,tweet) #[^\\x00-\\x19\\x21-\\x7F]+\n",
    "        new_tweet = \" \".join(new_tweet).replace('\\xa0','').replace('\\u200c','').replace('\\U000fe329','').replace('\\u2066','').replace('\\u2069','').strip()\n",
    "        X_dev_corrected_tweets.append(new_tweet)\n",
    "    return X_train_corrected_tweets, X_dev_corrected_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ced12cb2744cdebba909a36a7baf5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0685588fafa4d68a9624f4dba51c89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#3 country_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#3 country_label\"]\n",
    "\n",
    "X_train_corrected, X_dev_corrected = preprocess_text(X_train_original, X_dev_original)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_corrected)\n",
    "X_dev = vectorizer.transform(X_dev_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.53495277\n",
      "Iteration 2, loss = 1.23892702\n",
      "Iteration 3, loss = 0.27399245\n",
      "Iteration 4, loss = 0.10015557\n",
      "Iteration 5, loss = 0.06798964\n",
      "Iteration 6, loss = 0.05801829\n",
      "Iteration 7, loss = 0.05557359\n",
      "Iteration 8, loss = 0.05018162\n",
      "Iteration 9, loss = 0.04742837\n",
      "Iteration 10, loss = 0.04619383\n",
      "Iteration 11, loss = 0.04353102\n",
      "Iteration 12, loss = 0.04288444\n",
      "Iteration 13, loss = 0.04118913\n",
      "Iteration 14, loss = 0.04012071\n",
      "Iteration 15, loss = 0.03875515\n",
      "Iteration 16, loss = 0.03861753\n",
      "Iteration 17, loss = 0.03659112\n",
      "Iteration 18, loss = 0.03613538\n",
      "Iteration 19, loss = 0.03414700\n",
      "Iteration 20, loss = 0.03421354\n",
      "Iteration 21, loss = 0.03334296\n",
      "Iteration 22, loss = 0.03361130\n",
      "Iteration 23, loss = 0.03416203\n",
      "Iteration 24, loss = 0.03310318\n",
      "Iteration 25, loss = 0.03235669\n",
      "['Iraq' 'Egypt' 'Algeria' ... 'Egypt' 'Egypt' 'Sudan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "OVERALL SCORES:\r\n",
      "MACRO AVERAGE PRECISION SCORE: 14.75 %\r\n",
      "MACRO AVERAGE RECALL SCORE: 14.13 %\r\n",
      "MACRO AVERAGE F1 SCORE: 13.98 %\r\n",
      "OVERALL ACCURACY: 28.42 %\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier((1024,),verbose=True,max_iter=50)\n",
    "model.fit(X_train, y_train_original)\n",
    "prediction = model.predict(X_dev)\n",
    "print(prediction)\n",
    "write_result('tfidf_task1_unbal.txt',prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8885f3b6c24d7cba80e34f60757d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=93933.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f00dc5e3e541f795b52e622c3a69b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../tsv/oversample_train.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#3 country_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#3 country_label\"]\n",
    "\n",
    "X_train_corrected, X_dev_corrected = preprocess_text(X_train_original, X_dev_original)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_corrected)\n",
    "X_dev = vectorizer.transform(X_dev_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kuwait' 'Kuwait' 'Somalia' ... 'Egypt' 'Bahrain' 'Djibouti']\n",
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 11.46 %\n",
      "MACRO AVERAGE RECALL SCORE: 8.39 %\n",
      "MACRO AVERAGE F1 SCORE: 5.54 %\n",
      "OVERALL ACCURACY: 6.76 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier((1024,),verbose=True,max_iter=25)\n",
    "model.fit(X_train, y_train_original)\n",
    "prediction = model.predict(X_dev)\n",
    "print(prediction)\n",
    "write_result('tfidf_task1_bal.txt',prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66197dda090e4d9cac71d2480a31bccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ec55411c6d4dfeb688a75045183822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/train_labeled.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#4 province_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#4 province_label\"]\n",
    "\n",
    "X_train_corrected, X_dev_corrected = preprocess_text(X_train_original, X_dev_original)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_corrected)\n",
    "X_dev = vectorizer.transform(X_dev_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-e3a5f3c8898c>\", line 3, in <module>\n",
      "    prediction = model.predict(X_dev)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 971, in predict\n",
      "    y_pred = self._predict(X)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 685, in _predict\n",
      "    self._forward_pass(activations)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 104, in _forward_pass\n",
      "    self.coefs_[i])\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/utils/extmath.py\", line 151, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/posixpath.py\", line 396, in realpath\n",
      "    return abspath(path)\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/posixpath.py\", line 379, in abspath\n",
      "    if not isabs(path):\n",
      "  File \"/home/nikamanth/anaconda3/envs/torch/lib/python3.7/posixpath.py\", line 64, in isabs\n",
      "    def isabs(s):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier((1024,),verbose=True,max_iter=25)\n",
    "model.fit(X_train, y_train_original)\n",
    "prediction = model.predict(X_dev)\n",
    "print(prediction)\n",
    "write_result('tfidf_task2_unbal.txt',prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../tsv/oversample_train.tsv',sep='\\t')\n",
    "dev_df = pd.read_csv('../NADI-2020_release_1.0/NADI_release/dev_labeled.tsv',sep='\\t')\n",
    "\n",
    "X_train_original,y_train_original = train_df[\"#2 tweet_content\"],train_df[\"#4 province_label\"]\n",
    "X_dev_original,y_dev_original = dev_df[\"#2 tweet_content\"],dev_df[\"#4 province_label\"]\n",
    "\n",
    "X_train_corrected, X_dev_corrected = preprocess_text(X_train_original, X_dev_original)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_corrected)\n",
    "X_dev = vectorizer.transform(X_dev_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier((1024,),verbose=True,max_iter=25)\n",
    "model.fit(X_train, y_train_original)\n",
    "prediction = model.predict(X_dev)\n",
    "print(prediction)\n",
    "write_result('tfidf.txt_task2_bal',prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
